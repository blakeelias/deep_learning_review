{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"17dyNvcXkQObTwTojVJi1hoytNFOb6Yom","timestamp":1662139473026}],"collapsed_sections":[],"authorship_tag":"ABX9TyPGjOBjOte+hXymcq9srpJV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3NvPFCMr6glc"},"outputs":[],"source":["# Using torch as the underlying tensor library so that we can easily check if\n","# our gradients match the correct gradients.\n","\n","!pip3 install torch\n","import torch\n","import numpy as np"]},{"cell_type":"markdown","source":["See `autodiff.ipynb` for general purpose autodiff engine implementation.\n","\n","This notebook is a study reference for an interview which asks you to implement backprop for a feed-foward network. Here, we will skip the autodiff engine, and implement only what we need to write to pass such an interview."],"metadata":{"id":"5-2akqZWO_nR"}},{"cell_type":"code","source":["# Define some operations which will let us construct a feed foward network and loss.\n","# Each operation returns an output and a `vjp` function which can be called to calculate a backward-pass through the same op.\n","\n","def affine(x, w, b):\n","  y = x @ w + b\n","  \n","  def vjp(v):\n","    # w.r.t. x\n","    gx = v @ w.T\n","    \n","    # w.r.t. w\n","    gw = x.T @ v\n","    \n","    # w.r.t. b\n","    gb = v.sum(axis=0)\n","    \n","    return gx, gw, gb\n","  \n","  return y, vjp\n","\n","\n","def sigmoid(x):\n","  y = 1/(1+np.exp(-x))\n","  \n","  def vjp(v):\n","    dy = np.exp(x)/(1 + np.exp(x))**2\n","    return v*dy\n","  \n","  return y, vjp\n","\n","\n","def relu(x):\n","  y = np.clip(x, a_min=0, a_max=None)\n","\n","  def vjp(v):\n","    # Note that the torch implementation of relu makes the gradient 1 at x=0\n","    return v * (x >= 0).astype(float)\n","\n","  return y, vjp\n","\n","\n","def square_error_loss(x, *, y):\n","  diff = x - y\n","  L = (diff**2).sum(axis=-1)\n","  \n","  def vjp(v):\n","    return v * 2 * diff\n","  \n","  return L, vjp\n","\n","\n","def tensor_sum(x):\n","  y = x.sum()\n","  \n","  def vjp(v):\n","    return v * np.ones_like(x)\n","  \n","  return y, vjp"],"metadata":{"id":"RGwtNikW6wIi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Feed-forward network"],"metadata":{"id":"lbHkgazYknjl"}},{"cell_type":"code","source":["# build a simple NN with toy data\n","\n","# data - shape == (batch, features)\n","x = np.array([[1,2,3], [4,5,6]], dtype=float)\n","y = np.array([[1, 0], [0, 1]], dtype=float)\n","\n","# params\n","w1 = np.array([[1, 1], [-1, 1], [-2, 2]], dtype=float)\n","b1 = np.array([[0, 1]], dtype=float)\n","w2 = np.array([[.2, .5], [.5, -.5]], dtype=float)\n","b2 = np.array([[-1, .5]], dtype=float)\n","w3 = np.array([[.7, -.5], [-.2, .3]], dtype=float)\n","b3 = np.array([[.3, -.2]], dtype=float)\n","params = [w1, b1, w2, b2, w3, b3]\n","\n","# build NN\n","h0 = x\n","s1, s1_vjp = affine(h0, w1, b1)\n","h1, h1_vjp = sigmoid(s1)\n","s2, s2_vjp = affine(h1, w2, b2)\n","h2, h2_vjp = relu(s2)\n","s3, s3_vjp = affine(h2, w3, b3)\n","Le, Le_vjp = square_error_loss(s3, y=y)\n","L, L_vjp = tensor_sum(Le)\n","L  # view the output"],"metadata":{"id":"CjXzqIHP6x9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# perform backprop\n","g = L_vjp(1)\n","g = Le_vjp(g)\n","g, g_w3, g_b3 = s3_vjp(g)\n","g = h2_vjp(g)\n","g, g_w2, g_b2 = s2_vjp(g)\n","g = h1_vjp(g)\n","g, g_w1, g_b1 = s1_vjp(g)\n","\n","my_grads = [g_w1, g_b1, g_w2, g_b2, g_w3, g_b3]\n","my_grads  # view our grads"],"metadata":{"id":"vA0d8MXjQnZf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"w0097J54SCMt"}},{"cell_type":"code","source":["def sigmoid_torch(x):\n","  y = 1/(1+(-x).exp())\n","  return y, None\n","\n","\n","def relu_torch(x):\n","  # y = torch.maximum(x, torch.zeros_like(x))\n","  y = torch.clamp(x, min=0)\n","  return y, None"],"metadata":{"id":"UnDN1m1XTltL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w1t = torch.tensor(w1, requires_grad=True)\n","b1t = torch.tensor(b1, requires_grad=True)\n","w2t = torch.tensor(w2, requires_grad=True)\n","b2t = torch.tensor(b2, requires_grad=True)\n","w3t = torch.tensor(w3, requires_grad=True)\n","b3t = torch.tensor(b3, requires_grad=True)\n","torch_params = [w1t, b1t, w2t, b2t, w3t, b3t]\n","\n","h0 = torch.tensor(x)\n","s1, _ = affine(h0, w1t, b1t)\n","h1, _ = sigmoid_torch(s1)\n","s2, _ = affine(h1, w2t, b2t)\n","h2, _ = relu_torch(s2)\n","s3, _ = affine(h2, w3t, b3t)\n","Le, _ = square_error_loss(s3, y=torch.tensor(y))\n","L, _ = tensor_sum(Le)\n","L  # view the output"],"metadata":{"id":"47cgXo5sSL44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compare to gradients calculated by torch.\n","# This is for debugging purposes.\n","\n","def get_torch_grads(target, params):\n","  # zero out previous cum gradients\n","  for p in params:\n","    if p.grad is not None:\n","      p.grad.zero_()\n","  # update cum gradients\n","  target.backward(torch.ones_like(target), retain_graph=True)\n","  return [p.grad for p in params]\n","\n","\n","# compare to torch grads\n","torch_grads = get_torch_grads(L, torch_params)\n","print('matches:', [torch.allclose(torch.tensor(my_g), tc_g) for my_g, tc_g in zip(my_grads, torch_grads)])\n","# If all are True then we've succeeded"],"metadata":{"id":"Iy2bmrkK60-6"},"execution_count":null,"outputs":[]}]}