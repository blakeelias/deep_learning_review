{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "3NvPFCMr6glc"
      },
      "outputs": [],
      "source": [
        "# Using numpy as the underlying tensor library so that this impl is \"interview realistic.\"\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See `autodiff.ipynb` for general purpose autodiff engine implementation.\n",
        "\n",
        "This notebook is a study reference for an interview which asks you to implement backprop for a feed-foward network. Here, we will skip the autodiff engine, and implement only what we need to write to pass such an interview."
      ],
      "metadata": {
        "id": "5-2akqZWO_nR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Operations\n",
        "\n",
        "For each operation below, we want to provide a `grad` function. grad stands for vector-Jacobian product.\n",
        "\n",
        "Suppose we have a function $h = g\\circ f$ with $f : \\mathbb{R}^n\\to\\mathbb{R}^m$ and $g : \\mathbb{R}^m \\to \\mathbb{R}$. In the context of a neural network, $f$ would be a particular layer (or operation within a layer such as affine or relu/sigmoid), and $g$ would be the rest of the \"stack\" sitting on top of $f$, which goes through however many layers and then through the loss function, resulting in a final scalar loss. Let $x\\in \\mathbb{R}^n$. We are interested in the gradient $\\nabla_x h(x) = (\\partial h/\\partial x_1,\\dots,\\partial h/\\partial x_n)$, which using chain rule, is\n",
        "\n",
        "$$\n",
        "\\nabla h = \\nabla g J[f]\n",
        "$$\n",
        "\n",
        "where $z = f(x)$ and $\\nabla g = (\\partial g/\\partial z_1,\\dots,\\partial g/\\partial z_n)$ is a row-vector, and\n",
        "\n",
        "$$J[f] = \\begin{bmatrix}\\frac{\\partial f_1}{\\partial x_1}&\\dots&\\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots&\\ddots&\\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1}&\\dots&\\frac{\\partial f_m}{\\partial x_n}\\end{bmatrix}$$\n",
        "\n",
        "is a matrix of partial derivatives called the Jacobian. Hence, $\\nabla h$ is a vector-Jacobian product (grad).\n",
        "\n",
        "If there are layers of the network below $f$, we can just repeat this process until we get to the bottom. In other words, suppose $h' = g\\circ f\\circ f'$. Then we can let $g' = g\\circ f$ and $\\nabla g' = \\nabla h$, so that $\\nabla h' = \\nabla g'J[f']$.\n",
        "\n",
        "## Deriving grad Functions\n",
        "$\\newcommand{\\a}{\\alpha}\\newcommand{\\b}{\\beta}$\n",
        "\n",
        "We don't want to pass around Jacobians explicitly because they can be large and sparse. Instead, we pass around a `grad` function which calculates $\\nabla g J[f]$ while preforming whatever optimizations under the hood.\n",
        "\n",
        "Let's derive the `grad` function for the affine operation.\n",
        "\n",
        "Let $N$ be the batch size, $K_i$ be the number of input features, and $K_o$ be the number of output features.  \n",
        "Let $x \\in \\mathbb{R}^{N\\times K_i}$, $w \\in \\mathbb{R}^{K_i\\times K_o}$ and $b \\in \\mathbb{R}^{1\\times K_o}$ all be matrices.\n",
        "\n",
        "The affine operation is $$f(x, w, b) = xw+b$$\n",
        "\n",
        "Let $J_x[f], J_w[f]$ and $J_b[f]$ be the Jacobians w.r.t. $x$, $w$ and $b$.   \n",
        "These Jacobians are 4-tensors, with\n",
        "$$J_x[f]^{i,j}_{\\a, \\b} = \\frac{\\partial f_{i,j}}{\\partial x_{\\a, \\b}}$$\n",
        "etc., where  \n",
        "$J_x[f]$ has shape $(N, K_o \\mid N, K_i)$,  \n",
        "$J_w[f]$ has shape $(N, K_o \\mid K_i, K_o)$ and  \n",
        "$J_b[f]$ has shape $(N, K_o \\mid 1, K_o)$,  \n",
        "with $(\\text{output_shape} \\mid \\text{input_shape})$ denoting the dimensions of the raised followed by lowered tensor indices.\n",
        "\n",
        "Supposing we have a \"stack\" on top of $y = f(x,w,b)$ which outputs a scalar loss $\\mathcal{L} \\in \\mathbb{R}$, let $g = \\nabla_y \\mathcal{L} \\in \\mathbb{R}^{N\\times K_o}$ is a matrix (and 2-tensor) where $(\\nabla_y \\mathcal{L})_{i,j} = \\partial \\mathcal{L}/\\partial y_{i,j}$.\n",
        "\n",
        "Then\n",
        "\n",
        "$$\n",
        "\\text{grad}_x(g) = \\sum_{i, j} g_{i, j} J_x[f]^{i, j}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "\\text{grad}_w(g) = \\sum_{i, j} g_{i, j} J_w[f]^{i, j}\n",
        "$$\n",
        "\n",
        "etc.\n",
        "\n",
        "Lets find $J_x[f]$ and $J_w[f]$ explicitly. Since they are both 4-tensors, we won't be able (easily) visualize them in their entirety as grids. I find, in deriving the `grad` function, it is easist to work with $J[f]^{i,j}_{\\a, \\b}$\n",
        "\n",
        "So\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J_x[f]^{i,j}_{\\a,\\b} &= \\frac{\\partial}{\\partial x_{\\a,\\b}}(xw+b)_{i,j} \\\\\n",
        "  &= \\frac{\\partial}{\\partial x_{\\a,\\b}} (x_i \\cdot (w^T)_j + (b^T)_j) \\\\\n",
        "  &= \\begin{cases}w_{\\b,j} & i = \\a \\\\ 0 & \\text{otherwise} \\end{cases}\n",
        "\\end{aligned}$$\n",
        "\n",
        "and so\n",
        "\n",
        "$$\n",
        "\\text{grad}_x(g)_{\\a,\\b} = \\sum_{i, j} g_{i, j} J_x[f]^{i, j}_{\\a,\\b} = \\sum_{j} g_{\\a,j} w_{\\b,j}\n",
        "$$\n",
        "\n",
        "which we can rewrite as matrix multiply,\n",
        "\n",
        "$$\\text{grad}_x(g) = gw^T$$\n",
        "\n",
        "Likewise we have,\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J_w[f]^{i,j}_{\\a,\\b} &= \\frac{\\partial}{\\partial w_{\\a,\\b}}(xw+b)_{i,j} \\\\\n",
        "  &= \\frac{\\partial}{\\partial w_{\\a,\\b}} (x_i \\cdot (w^T)_j + (b^T)_j) \\\\\n",
        "  &= \\begin{cases}x_{i,\\a} & j = \\b \\\\ 0 & \\text{otherwise} \\end{cases}\n",
        "\\end{aligned}$$\n",
        "\n",
        "giving us\n",
        "\n",
        "$$\n",
        "\\text{grad}_w(g)_{\\a,\\b} = \\sum_{i, j} g_{i, j} J_w[f]^{i, j}_{\\a,\\b} = \\sum_{i} g_{i,\\b} x_{i,\\a}\n",
        "$$\n",
        "\n",
        "which we can rewrite as the matrix multiply,\n",
        "\n",
        "$$\\text{grad}_w(g) = x^T g$$"
      ],
      "metadata": {
        "id": "LD41hfP7VyYS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xyjsipQqko9g"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In code, for any variable `v`, we will use `gv` to represent the gradient of the loss `L` with respect to `v`.\n",
        "\n",
        "I.e. `gv` = $\\nabla_{v}L = \\frac{\\partial{L}}{\\partial{v}}$"
      ],
      "metadata": {
        "id": "7SGlLErHkpfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some operations which will let us construct a feed foward network and loss.\n",
        "# Each operation returns an output and a `grad` function which can be called to calculate a backward-pass through the same op.\n",
        "\n",
        "# In code, for any variable `v`, we will use `gv` to represent the gradient of the loss `L` with respect to `v`.\n",
        "# I.e. `gv` = $\\nabla_{v}L = \\frac{\\partial{L}}{\\partial{v}}$\n",
        "\n",
        "def affine(x, w, b):\n",
        "  y = x @ w + b\n",
        "\n",
        "  def grad(gy):\n",
        "    # w.r.t. x\n",
        "    gx = gy @ w.T\n",
        "\n",
        "    # w.r.t. w\n",
        "    gw = x.T @ gy\n",
        "\n",
        "    # w.r.t. b\n",
        "    gb = gy.sum(axis=0)\n",
        "\n",
        "    return gx, gw, gb\n",
        "\n",
        "  return y, grad\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "  y = 1/(1+np.exp(-x))\n",
        "\n",
        "  def grad(gy):\n",
        "    # Why is this written with positive x or negative x?\n",
        "    # Numerical stability?\n",
        "    # Play with this, try with PyTorch test.\n",
        "    # dy = np.exp(-x)/(1 + np.exp(-x))**2\n",
        "    dy = y * (1 - y)\n",
        "    return gy*dy\n",
        "\n",
        "  return y, grad\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "  y = np.clip(x, a_min=0, a_max=None)\n",
        "\n",
        "  def grad(gy):\n",
        "    # Note that the torch implementation of relu makes the gradient 1 at x=0\n",
        "    return gy * (x >= 0).astype(float)\n",
        "\n",
        "  return y, grad\n",
        "\n",
        "\n",
        "def square_error_loss(x, y):\n",
        "  diff = x - y\n",
        "  l = (diff**2).sum(axis=-1)\n",
        "\n",
        "  def grad(gl):\n",
        "    return gl * 2 * diff\n",
        "\n",
        "  return l, grad\n",
        "\n",
        "\n",
        "def tensor_sum(x):\n",
        "  y = x.sum()\n",
        "\n",
        "  def grad(gy):\n",
        "    return gy * np.ones_like(x)\n",
        "\n",
        "  return y, grad"
      ],
      "metadata": {
        "id": "RGwtNikW6wIi"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feed-forward network"
      ],
      "metadata": {
        "id": "lbHkgazYknjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build a simple NN with toy data\n",
        "\n",
        "# data - shape == (batch, features)\n",
        "x = np.array([[1,2,3], [4,5,6]], dtype=float)\n",
        "y = np.array([[1, 0], [0, 1]], dtype=float)\n",
        "\n",
        "# params\n",
        "w1 = np.array([[1, 1], [-1, 1], [-2, 2]], dtype=float)\n",
        "b1 = np.array([[0, 1]], dtype=float)\n",
        "w2 = np.array([[.2, .5], [.5, -.5]], dtype=float)\n",
        "b2 = np.array([[-1, .5]], dtype=float)\n",
        "w3 = np.array([[.7, -.5], [-.2, .3]], dtype=float)\n",
        "b3 = np.array([[.3, -.2]], dtype=float)\n",
        "params = [w1, b1, w2, b2, w3, b3]\n",
        "\n",
        "# build NN\n",
        "h0 = x\n",
        "s1, s1_grad = affine(h0, w1, b1)\n",
        "h1, h1_grad = sigmoid(s1)\n",
        "s2, s2_grad = affine(h1, w2, b2)\n",
        "h2, h2_grad = relu(s2)\n",
        "s3, s3_grad = affine(h2, w3, b3)\n",
        "Le, Le_grad = square_error_loss(s3, y=y)\n",
        "L, L_grad = tensor_sum(Le)\n",
        "L  # view the output"
      ],
      "metadata": {
        "id": "CjXzqIHP6x9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3928157b-4a66-4c53-be03-66628bfe826f"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(2.060075596202683)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# perform backprop\n",
        "g = L_grad(1)\n",
        "g = Le_grad(g)\n",
        "g, g_w3, g_b3 = s3_grad(g)\n",
        "g = h2_grad(g)\n",
        "g, g_w2, g_b2 = s2_grad(g)\n",
        "g = h1_grad(g)\n",
        "_, g_w1, g_b1 = s1_grad(g)\n",
        "\n",
        "my_grads = [g_w1, g_b1, g_w2, g_b2, g_w3, g_b3]\n",
        "my_grads  # view our grads"
      ],
      "metadata": {
        "id": "vA0d8MXjQnZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7750ccd-9cce-4883-bde9-c1546064d9a2"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[ 6.90769472e-05, -3.63401821e-06],\n",
              "        [ 1.41001896e-04, -7.26838789e-06],\n",
              "        [ 2.12926844e-04, -1.09027576e-05]]),\n",
              " array([ 7.19249484e-05, -3.63436968e-06]),\n",
              " array([[ 0.00000000e+00,  1.43982798e-04],\n",
              "        [ 0.00000000e+00, -6.79882637e-01]]),\n",
              " array([ 0.        , -0.67987537]),\n",
              " array([[ 0.        ,  0.        ],\n",
              "        [-0.00066893, -0.00019387]]),\n",
              " array([-0.80019174, -2.79971239])]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "w0097J54SCMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare our gradients against gradients calculated by PyTorch.\n",
        "!pip3 install torch\n",
        "import torch"
      ],
      "metadata": {
        "id": "HmG_0_OvVncJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410b5c71-cd56-4ac6-adfb-ef3cec3395ed"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_torch(x):\n",
        "  y = 1/(1+(-x).exp())\n",
        "  return y, None\n",
        "\n",
        "\n",
        "def relu_torch(x):\n",
        "  # y = torch.maximum(x, torch.zeros_like(x))\n",
        "  y = torch.clamp(x, min=0)\n",
        "  return y, None"
      ],
      "metadata": {
        "id": "UnDN1m1XTltL"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1t = torch.tensor(w1, requires_grad=True)\n",
        "b1t = torch.tensor(b1, requires_grad=True)\n",
        "w2t = torch.tensor(w2, requires_grad=True)\n",
        "b2t = torch.tensor(b2, requires_grad=True)\n",
        "w3t = torch.tensor(w3, requires_grad=True)\n",
        "b3t = torch.tensor(b3, requires_grad=True)\n",
        "torch_params = [w1t, b1t, w2t, b2t, w3t, b3t]\n",
        "\n",
        "h0 = torch.tensor(x)\n",
        "s1, _ = affine(h0, w1t, b1t)\n",
        "h1, _ = sigmoid_torch(s1)\n",
        "s2, _ = affine(h1, w2t, b2t)\n",
        "h2, _ = relu_torch(s2)\n",
        "s3, _ = affine(h2, w3t, b3t)\n",
        "Le, _ = square_error_loss(s3, y=torch.tensor(y))\n",
        "L, _ = tensor_sum(Le)\n",
        "L  # view the output"
      ],
      "metadata": {
        "id": "47cgXo5sSL44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c9b51e-51f0-41e4-a345-b5336bf54ff9"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.0601, dtype=torch.float64, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare to gradients calculated by torch.\n",
        "# This is for debugging purposes.\n",
        "\n",
        "def get_torch_grads(target, params):\n",
        "  # zero out previous cum gradients\n",
        "  for p in params:\n",
        "    if p.grad is not None:\n",
        "      p.grad.zero_()\n",
        "  # update cum gradients\n",
        "  target.backward(torch.ones_like(target), retain_graph=True)\n",
        "  return [p.grad for p in params]\n",
        "\n",
        "\n",
        "# compare to torch grads\n",
        "torch_grads = get_torch_grads(L, torch_params)\n",
        "print('matches:', [torch.allclose(torch.tensor(my_g), tc_g) for my_g, tc_g in zip(my_grads, torch_grads)])\n",
        "# If all are True then we've succeeded"
      ],
      "metadata": {
        "id": "Iy2bmrkK60-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfb584c-d71f-495f-cceb-d8117bdb8c42"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matches: [True, True, True, True, True, True]\n"
          ]
        }
      ]
    }
  ]
}