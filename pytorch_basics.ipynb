{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch_basics.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPE9j/5JTv1DUeBok7lEn2g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This is a demonstration of the \"raw\" PyTorch API.  \n","We get to see manual parameter creation, manual graph building, and manual gradient updates.  \n","After seeing how PyTorch works, we can start throwing on helpers, wrappers, and managers for convenience and performance (like running operations on GPU)."],"metadata":{"id":"ZiVtszEDnNNL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tourn-5CK0tN"},"outputs":[],"source":["!pip3 install torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.transforms as transforms\n","print(torch.__version__)"]},{"cell_type":"markdown","source":["# Load the data"],"metadata":{"id":"zB5O7efRM9bK"}},{"cell_type":"code","source":["# Download MNIST dataset and parse into pytorch Dataset objects.\n","\n","trainset = torchvision.datasets.MNIST(root='./data', download=True, train=True)\n","testset = torchvision.datasets.MNIST(root='./data', download=True, train=False)"],"metadata":{"id":"Kp9Jm80ZLAGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How big is our dataset? What kind of data do we have? \n","\n","print(trainset.data.shape, ', ', trainset.data.dtype)\n","print(trainset.targets.shape, ', ', trainset.targets.dtype)\n","print()\n","print(testset.data.shape, ', ', testset.data.dtype)\n","print(testset.targets.shape, ', ', testset.targets.dtype)"],"metadata":{"id":"nGpEg46gLRKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Look at an example\n","# Images are monochrome with integer pixel values between 0 and 255 (inclusive)\n","\n","torch.set_printoptions(linewidth=1000)\n","print(trainset.data[0])"],"metadata":{"id":"f-5f7I9dM7fx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize some images and check their labels\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","for i in range(6):\n","  print(trainset.targets[i].numpy())\n","  plt.imshow(trainset.data[i].numpy())\n","  plt.show()\n","  print('')"],"metadata":{"id":"yDMnZoy0NAuK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform the dataset:\n","#   cast type to float\n","#   rescale pixels to [0, 1]\n","#   flatten each image, i.e. reshape from (28,28) to (,784)\n","\n","trainset.data = trainset.data.flatten(1) / 255\n","testset.data = testset.data.flatten(1) / 255\n","\n","print(trainset.data.shape)\n","print(trainset.data.dtype)\n","print(trainset.data.max())\n","print()\n","print(testset.data.shape)\n","print(testset.data.dtype)\n","print(testset.data.max())"],"metadata":{"id":"M0JqPlM3VzTa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define the model"],"metadata":{"id":"kTkui1mdOpyF"}},{"cell_type":"code","source":["INPUT_DIM = trainset.data.shape[-1]\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 10\n","\n","# Define and initialize the model parameters by sampling each element i.i.d. from a normal distribution\n","SCALE = 1e-2\n","W1 = torch.nn.Parameter(SCALE * torch.normal(mean=0, std=1, size=(INPUT_DIM, HIDDEN_DIM)))\n","W2 = torch.nn.Parameter(SCALE * torch.normal(mean=0, std=1, size=(HIDDEN_DIM, OUTPUT_DIM)))\n","B1 = torch.nn.Parameter(SCALE * torch.normal(mean=0, std=1, size=(1, HIDDEN_DIM)))\n","B2 = torch.nn.Parameter(SCALE * torch.normal(mean=0, std=1, size=(1, OUTPUT_DIM)))\n","\n","parameters = [W1, W2, B1, B2]\n","\n","def model_fn(x):\n","  # x.shape=(batch_size, INPUT_DIM)\n","  h = F.relu(torch.matmul(x, W1) + B1)\n","  return torch.matmul(h, W2) + B2"],"metadata":{"id":"sjkQbPmONYdN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test out our model\n","\n","out = model_fn(trainset.data[:23])\n","print(out.shape)\n","print(out)"],"metadata":{"id":"ck5R2qXoWuVJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# View our parameters\n","\n","print(W1)\n","print(B1)"],"metadata":{"id":"w4KbBebKVS6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the loss function\n","\n","def select_indices(x, indices):\n","  # From row i, select element indices[i], i.e.\n","  # return [x[i, indices[i]] for i in range(x.shape[0])]\n","  return x.gather(1, indices[:, None])\n","\n","# Maximum likelihood loss (negative log probability of the data)\n","def mle_loss(x, labels):\n","  # equivalent to cross entropy loss where target probs are 1 on the correct labels\n","  # https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy\n","  logits = F.log_softmax(x, dim=1)  # rescale outputs in log space\n","  return -select_indices(logits, labels).mean()"],"metadata":{"id":"OKWTl7eTXig3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Cross entropy loss***\n","\n","See https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy.\n","\n","Let $\\vec{q} = (q_1, \\dots, q_n)$ be a vector of predicted probabilities,  \n","and let $\\vec{p} = (p_1, \\dots, p_n)$ be a vector of target probabilities.\n","\n","The [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) of the two distributions is\n","\n","$$\n","H(\\vec{p}, \\vec{q}) = -\\sum_{i=1}^n p_i \\log q_i\n","$$\n","\n","Holding $\\vec{p}$ fixed, $H(\\vec{p}, \\vec{q})$ is maximized when $\\vec{q} = \\vec{p}$.\n","\n","In our case, the labels provide a one-hot target distribution.  \n","Let $t$ be the target label for some input image.\n","A one-hot distribution puts all probability on $t$, i.e.\n","\n","$$\\vec{\\mathbb{1}}[t] = (0,\\dots,0,1,0,\\dots,0)$$\n","\n","where $\\mathbb{1}[t]_t = 1$.\n","Let $\\vec{Y}$ be the raw model outputs and $\\vec{q}$ be the model probabilities, i.e. \n","\n","$$\n","\\vec{q} = \\text{softmax}(\\vec{Y}) = \\frac{1}{\\sum_{i=1}^n \\exp(Y_i)}\\Big(\\exp(Y_1),\\dots,\\exp(Y_n)\\Big)\n","$$\n","\n","and let $\\vec{p} = \\vec{\\mathbb{1}}[t]$.\n","\n","\n","Then the cross entropy loss (negated so that minimizing maximizes cross entropy) is\n","\n","\n","$$\\begin{aligned}\n","L &= -H(\\vec{p}, \\vec{q}) \\\\\n","&= -H\\left(\\vec{\\mathbb{1}}[t],\\ \\text{softmax}(\\vec{Y})\\right) \\\\\n","&= \\sum_{i=1}^n \\mathbb{1}[t]_i \\log\\left( \\text{softmax}(\\vec{Y}) \\right)\\\\\n","&= \\log\\left( \\text{softmax}(\\vec{Y})_t \\right) \\\\\n","&= Y_t - \\log\\left(\\sum_{i=1}^n \\exp(Y_i)\\right)\n","\\end{aligned}$$\n","\n"],"metadata":{"id":"kiLP8m-kZx3B"}},{"cell_type":"code","source":["def accuracy(logits, target):\n","  argmaxs = logits.max(1).indices\n","  corrects = torch.eq(argmaxs, target)\n","  return corrects.float().mean()"],"metadata":{"id":"J3RYqmJ0daFp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training loop"],"metadata":{"id":"jWXOaklsmSB2"}},{"cell_type":"code","source":["batch_size = 100\n","learning_rate = 1e-2\n","num_epochs = 1000\n","regularizer_weight = 1e-2\n","\n","for epoch in range(batch_size):\n","  idx = torch.randperm(trainset.data.shape[0])  # random ordering of the training set\n","\n","  ## training step\n","  for i in range(0, trainset.data.shape[0], batch_size):\n","    x = trainset.data[idx[i:i+batch_size]]\n","    y = trainset.targets[idx[i:i+batch_size]]\n","\n","    ## forward + backprop + loss\n","    logits = model_fn(x)\n","    loss = mle_loss(logits, y)\n","    loss += regularizer_weight * sum(torch.linalg.norm(p, 1) for p in parameters)  # regularization\n","    loss.backward()  # calculate gradients\n","\n","    # apply gradient updates to parameters\n","    for p in parameters:\n","      p.data.sub_(p.grad.data * learning_rate)\n","      p.grad.zero_()  # zero out gradient, otherwise they accumulate across multiple `backward` calls.\n","\n","  train_logits = model_fn(trainset.data)  # Training accurate\n","  test_logits = model_fn(testset.data)  # Test accuracy\n","  loss = mle_loss(train_logits, trainset.targets)\n","  print('Epoch: %d | Train Loss: %.4f | Train Accuracy: %.2f | Test Accuracy: %.2f' % (epoch, loss.detach().item(), accuracy(train_logits, trainset.targets), accuracy(test_logits, testset.targets)))\n"],"metadata":{"id":"Aoma5Ny6dd9O"},"execution_count":null,"outputs":[]}]}