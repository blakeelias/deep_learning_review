{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPDwa0xRNi8nCLopz31R7D4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3NvPFCMr6glc"},"outputs":[],"source":["# Using torch as the underlying tensor library so that we can easily check if\n","# our gradients match the correct gradients.\n","\n","!pip3 install torch\n","import torch\n","print(torch.__version__)"]},{"cell_type":"markdown","source":["# Design\n","\n","The goal is to build an eager-mode backward-mode autodiff engine, similar to torch. This will be an ultra bare-bones implementation for pedagogical purposes.\n","\n","## Terminology\n","\n","Eager-mode means that for each computation of outputs and gradients we build a new graph (this makes it easy to create dynamic graphs structures like loops using the host language, Python). Backward-mode means we calculate gradients of parameters with a backward pass through the computation graph, from output to leaves. This is what \"backprop\" refers to. Autodiff ([automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)), distinct from [symbolic differentiation](https://en.wikipedia.org/wiki/Symbolic_differentiation \"Symbolic differentiation\") and [numerical differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation \"Numerical differentiation\"), uses the symbolic structure of a computation graph to calculate symbolically exact derivatives at a given input (the result is not symbolic but numerical, and we are also not performing a numerical estimation but doing an exact calculation up to rounding error).\n","\n","## Design Of An Autodiff Engine\n","\n","Let's assume we have access to some pre-existing tensor library, i.e. a library supports multi-dimensional array operations. In this demo, we are using Torch, which has the benefit of also being an autodiff engine so that we can easily compare our calculated gradients to the correct gradients for testing purposes.\n","\n","Suppose `x` is some tensor and `F` is a function that takes in a tensor and outputs a tensor. Then `y = F(x)` is a tensor. However, we want to record that we called `F` on `x` to produce `y`. So let's have `F` return `x` wrapped in a graph node instance which stores `F` and points to `x` as a child. Then `y` is this node object wrapping the output tensor.\n","\n","Suppose `G` is another such tensor function so that `z = G(y)` is a node instance containing the result of the computation `G(F(x))` and also points to the node `y` as its child.\n","\n","In general we might be interested in the Jacobian of `z` w.r.t. `x`, i.e. the tensor of all partial derivatives of elements in `z` w.r.t. elements in `x`, evaluated at `x`. However, we will restrict ourselves to calculating gradients where `z` is a scalar (1-tensor). This is the typical use case in machine learning, where `z` is a loss, and this is also computationally simpler to deal with.\n","\n","When `z` is a scalar, the Jacobian of `z` w.r.t. `x` is the tensor of partial derivatives of `z` w.r.t. each element of `x`, called the gradient. This gradient is the same shape as `x`.\n","\n","\n","### Tensor Chain Rule\n","\n","Let $f$ and $g$ be tensor-valued functions.  \n","The *shape* of a tensor is a tuple of numbers, each referring to the number of positions in each tensor dimension.  \n","Let $J$ be the Jacobian operator, so that e.g. $J[f]$ is the Jacobian of $f$ w.r.t. its input.\n","\n","Suppose,  \n","$f$ has input shape $(n_1,\\dots,n_r)$ and output shape $(m_1,\\dots,m_s)$, and    \n","$g$ has input shape $(m_1,\\dots,m_s)$ and output shape $(p_1,\\dots,p_t)$.\n","\n","\n","$J[f]$ has shape $(m_1,\\dots,m_s \\mid n_1,\\dots,n_r)$ (tensor consisting of partial derivatives of every output dimension w.r.t. every input dimension).    \n","$J[g]$ has shape $(p_1,\\dots,p_t \\mid m_1,\\dots,m_s)$.  \n","$J[g\\circ f]$ must have shape $(p_1,\\dots,p_t \\mid n_1,\\dots,n_r)$.  \n","Here I am distinguishing between input and output indices in the Jacobian with a middle bar.\n","\n","Now we can write the tensor chain rule:  \n","Letting lower indices be input indices, and upper indices be output indices, we have\n","$$J[g\\circ f]_{i_1,\\dots,i_r}^{j_1,\\dots,j_t} = \\sum_{k_1,\\dots,k_s} J[g]_{k_1,\\dots,k_s}^{j_1,\\dots,j_t}J[f]_{i_1,\\dots,i_r}^{k_1,\\dots,k_s}$$\n","\n","Or written out in terms of partial derivatives, $\\newcommand{\\pd}{\\partial}\\newcommand{\\pdiff}[2]{\\frac{\\pd{#1}}{\\pd{#2}}}$\n","\n","$$\n","\\pdiff{(g(f(x))^{j_1,\\dots,j_t})}{(x_{i_1,\\dots,i_r})} = \\sum_{k_1,\\dots,k_s} \\pdiff{(g(f(x))^{j_1,\\dots,j_t})}{(f(x)^{k_1,\\dots,k_s})}\\pdiff{(f(x)^{k_1,\\dots,k_s})}{(x_{i_1,\\dots,i_r})}\n","$$\n","\n","### Design of backward-mode differentiation\n","\n","From the tensor chain rule, we can see how autodiff would work. Each graph node, in addition to storing its input children, output, and function called, should also store a Jacobian. \n","\n","However, producing explicit Jacobians and summing over them is often unnecessarily costly, since they will tend to sparse 4-tensors (most entries are 0s). The conventional solution to this problem is to have each graph node provide a `backward` function which takes in a gradient tensor (w.r.t. that node's output tensor) and produces a new gradient tensor (w.r.t. that node's input tensor) performing whatever optimizations are necessary during the computation under the hood.\n","\n","Thus this autodiff engine design only supports gradients, i.e. derivatives w.r.t. a scalar output, and does not in general produce explicit Jacobian tensors. But this is sufficient for our purposes. While Jacobian functions are provided in libraries like TensorFlow and Torch, they are not often invoked in deep learning applications.\n","\n","#### Vector-Jacobian Product (VJP)\n","\n","When $g\\circ f$ outputs a scalar, then the tensor chain rule reduces to \n","\n","$$J[g\\circ f]_{i_1,\\dots,i_r} = \\sum_{k_1,\\dots,k_s} J[g]_{k_1,\\dots,k_s}J[f]_{i_1,\\dots,i_r}^{k_1,\\dots,k_s}$$\n","\n","where $J[f]$ has shape $(m_1,\\dots,m_s \\mid n_1,\\dots,n_r)$ and $J[g]$ has shape $(1 \\mid m_1,\\dots,m_s)$.\n","\n","If we flatten these tensors so that $J[f]$ has shape $(m \\mid n)$ and $J[g]$ has shape $(1 \\mid m)$, then the chain rule becomes a matrix product:\n","\n","$$J[g\\circ f] = J[g]J[f]$$\n","\n","where $J[f]$ is an $m\\times n$ matrix and $J[g]$ is a $1\\times m$ matrix (a row-vector). Hence we have a vector-Jacobian product (as opposed to Jacobian-vector product, the reverse order of the Jacobian times a column vector, which is used in forward-mode autodiff, see [this article](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#how-it-s-made-two-foundational-autodiff-functions) for details).\n","\n","Again, doing this product with an explicit Jacobian is needlessly compute intensive, since that Jacobian matrix is likely very sparse. What is done in practice, and what we will do, is have each computation node store a `vjp` function which takes in the \"vector\" $J[g]$ (which is just a gradient since $g$ outputs a scalar) and outputs $J[g\\circ f]$ (which is also just a gradient).\n","\n","Specifically, if our tensor-valued function `F` has input shape $(n_1,\\dots,n_r)$ and output shape $(m_1,\\dots,m_s)$, then the function call `F(x)` should return a computation node which contains `x`, `F`, and `vjp`, where `vjp` is a tensor-valued function with input shape $(m_1,\\dots,m_s)$ and output shape $(n_1,\\dots,n_r)$ (reverse of `F`). When called, we have `g_in = vjp(g_out)`, where `g_out` is the gradient w.r.t. some scalar output at the top of our computation graph, and `g_in` is the gradient w.r.t. that same scalar going through the call `F` evaluated at `x`. \"Gradient\" here means a tensor.\n","\n","More specifically, let our computation graph be the chain $g = f_r\\circ \\dots\\circ f_1$ where $f_r$ outputs a scalar,  and let $y = g(x)$ with $h_i = f_i \\circ \\dots \\circ f_1(x)$ so that $y = h_r$. Then $J[f_r\\circ \\dots\\circ f_{i+1}]$ has shape $(1\\mid \\text{shape}(h_i))$, which we are calling the gradient of $f_r\\circ \\dots\\circ f_{i+1}$ w.r.t. its input $h_i$. The function `vjp` at node $i$ in this graph takes in $J[f_r\\circ \\dots\\circ f_{i+1}]$ with shape $(1\\mid \\text{shape}(h_i))$ and outputs $J[f_r\\circ \\dots\\circ f_{i}]$ with shape $(1\\mid \\text{shape}(h_{i-1}))$. So the explicit gradient tensors flowing through the graph (from top to bottom) are not very large Jacobians, and we get no combinatorial explosion. We avoid explicitly passing around $J[f_i]$ with shape $(\\text{shape}(h_{i}) \\mid \\text{shape}(h_{i-1}))$, which may be a much larger tensor.\n","\n","\n","#### Meta Chain Rule\n","\n","So far we've only considered computation graphs which are chains. Extending our autodiff engine to arbitrary DAGs is straightforward.\n","\n","The multivariate chain-rule states that for $f(x(z),y(z))$, we have\n","\n","$$\n","\\frac{\\text{d} f}{\\text{d} z} = \\frac{\\partial f}{\\partial x}\\frac{\\text{d} x}{\\text{d} z} + \\frac{\\partial f}{\\partial y}\\frac{\\text{d} y}{\\text{d} z}\n","$$\n","\n","which extends to more than two arguments in the way you'd expect.\n","\n","The tensor chain rule contains this logic within it if we consider $f$ to take a single tensor argument which contains two elements.\n","\n","However, what about the case when $f$ takes as input multiple tensor arguments? What I call the meta chain rule, the multivariate chain rule for scalars extends to a multivariate chain rule for tensors in the obvious way: letting $f,x,y$ be tensor-valued functions and $z$ be a tensor, we have\n","\n","$$J_{z}[f(x(z),y(z))] = \\sum_{\\text{indices} \\in \\text{shape}(x)} J_x[f]_{\\text{indices}}J_z[x]^{\\text{indices}}+\\sum_{\\text{indices} \\in \\text{shape}(x)}J_y[f]_{\\text{indices}}J_z[y]^{\\text{indices}}$$\n","\n","etc.  \n","which we can notationally simplify to using [Einstein summation notation](https://en.wikipedia.org/wiki/Einstein_notation) (where matching upper and lower indices are automatically summed over):\n","\n","$$J_{z}[f(x(z),y(z))] = J_x[f]J_z[x]+J_y[f]J_z[y]$$\n","\n","What this amounts to for our design is the following:\n","\n","Suppose `F` takes as input a tuple of inputs and gives a tuple of outputs, all elements being tensors. E.g. `a, b = F(x, y)`. Then we want `a` and `b` to each be distinct graph node instances. Each node contains a single output, but points to many child nodes as inputs. So `a` and `b` both point to `x` and `y` as children. However, `a` and `b` will have different `vjp` functions due to the gradients \"passing through\" different paths, i.e. through `a` to `(x,y)` vs through `b` to `(x,y)`.\n","\n","### Graph Traversals\n","\n","How to traverse our computation graph and accumulate gradients is the final piece of the puzzle.\n","\n","Suppose we are calculating the gradient of some top scalar node `y` w.r.t. some leaf node `x`. Further suppose `x` is an input to many different nodes in the graph. Every node where `x` is an input has its own `vjp` function.\n","\n","Starting from `y` we do a directed graph traversal (BFS or DFS or whatever), going from parent to children, so that we traverse each directed edge exactly once. Whenever we hit a leaf node `x` (tensor instance), we at that point will have produced the gradient of `y` output w.r.t. `x` **via that path from `y` to `x`**. The meta chain rule simply tells us to sum the gradients across all distinct edges which point to that same `x` (so every place `x` appears as a child to any node in the graph). Thus we simply accumulate a running sum of gradients whenever we hit `x` via a distinct edge. Once we've completed the traversal, we return the accumulated gradient sum.\n","\n","We can just as easily calculate gradients w.r.t. multiple different input tensors, e.g. `x_1`, ..., `x_k`. We have a separate gradient accumulation for each such dinstinct tensor instance and add to the corresponding accumulation whenever we hit the tensor instance.\n","\n","\n","### Other Design Considerations\n","\n","We want the ability for ops to take construction-time arguments as well as inputs. For example, `F(x, activation=s)` passes in an activation function. Or, e.g. `G(F(x), flag=True)`. These auxilliary arguments are hyperparameters and not intended to be differentiable, so we don't want our autodiff engine to attempt to \"pass gradients through\" them. Using Python syntactic sugar, we can requires that `F`, `G`, etc. produce graph nodes which contain `vjp` functions that ignore these keyword arguments, and have the graph structure not reflect these keyword arguments.\n","\n","## Further Reading\n","\n","I obtained the bulk of my understanding of how to implement autodiff, and specifically vector-Jacobian products, from the [JAX docs](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#how-it-s-made-two-foundational-autodiff-functions).\n","\n","PyTorch also has some exposition of autodiff in [their blog post](https://pytorch.org/blog/overview-of-pytorch-autograd-engine/). The references at the bottom of their post seem useful as well.\n","\n","For a more comprehensive pedagogical implementation of autodiff, see  \n","https://github.com/mattjj/autodidact\n","\n","\n","\n"],"metadata":{"id":"kjf9OAOE7B-I"}},{"cell_type":"markdown","source":["# Implementation"],"metadata":{"id":"uaGIpeKC7Ds3"}},{"cell_type":"code","source":["# Decorator for defining node computation functions from \"regular\" tensor valued functions.\n","# This point of this is so we can define graph operations entirely in terms of torch tensors,\n","# without having to think about ComputeNode objects.\n","# However, we still need to explicitly define corresponding `vjp` functions for the backward pass.\n","def graph_computation(func):\n","  def wrapper(*args, **kwargs):\n","    # args should be ComputeNode or tensor instances.\n","    # kwargs are hyperparameters which are not differentiable.\n","    \n","    # Unwrap any ComputeNode arguments and pass their output tensors along as input to this function call.\n","    unwrapped = [(arg.output if isinstance(arg, ComputeNode) else arg) for arg in args]\n","    results, vjps = func(*unwrapped, **kwargs)\n","    # We expect `func` to return both output tensors and a `vjp` function for each output.\n","\n","    if not isinstance(results, (tuple, list)):\n","      results = [results]\n","      vjps = [vjps]\n","    output = [ComputeNode(output=r, inputs=args, vjp=vjp, op=func, kwargs=kwargs) for r, vjp in zip(results, vjps)]\n","    if len(output) == 1: output = output[0]\n","\n","    # Return the output tensor(s) wrapped in `ComputeNode` objects containing all the correct information.\n","    # Returns a single `ComputeNode` if `func` returns a single output, or a list of `ComputeNode` if `func` returns a list of outputs.\n","    return output\n","  return wrapper\n","\n","\n","class ComputeNode:\n","  \n","  def __init__(self, *, output, inputs, vjp, op, kwargs):\n","    # leaf node if `inputs` is None\n","    self.output = output\n","    self.inputs = inputs\n","    self.vjp = vjp\n","    self.op = op\n","    self.kwargs = kwargs  # keyword dictionary of hyperparameters\n","    # `op` should be a pure function (stateless) so that we can reproduce `output`\n","    # given `op`, `inputs` and `kwargs`.\n","    self._grads = None   # intermediate gradients for debugging purposes\n","    \n","  def backprop(self, targets, grad=None, cum_gradients=None):\n","    # calculate jacobian of this output w.r.t. a list of ComputeNode instances\n","    # recursively calls backprop on input nodes - assumes no directed cycles\n","    \n","    if grad is None:\n","      grad = torch.ones_like(self.output)\n","    if cum_gradients is None:\n","      return_grads = True\n","      cum_gradients = {id(v): torch.zeros_like(v) for v in targets}\n","    else:\n","      return_grads = False\n","      \n","    input_grads = self._grads = self.vjp(grad)\n","\n","    if not isinstance(input_grads, (list, tuple)):\n","      input_grads = [input_grads]\n","    for a, g in zip(self.inputs, input_grads):\n","      if torch.is_tensor(a):\n","        if id(a) in cum_gradients:\n","          cum_gradients[id(a)] += g\n","      else:  # instance of ComputeNode\n","        a.backprop(targets, g, cum_gradients)\n","        \n","    if return_grads:\n","      return [cum_gradients[id(t)] for t in targets]\n","    \n","  def __repr__(self):\n","    return repr(self.output)\n","  "],"metadata":{"id":"fr479l1a6r-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define some operations which will let us construct a feed foward network and loss.\n","\n","@graph_computation\n","def affine(x, w, b):\n","  # output\n","  y = x @ w + b\n","  \n","  # vjp function\n","  def vjp(v):\n","    # calculate vJ, where v is a row vector and J is the jacobian matrix of this calculuation,\n","    # with J having shape (prod(output_shape), prod(input_shape))\n","    \n","    # w.r.t. x\n","    gx = v @ w.T\n","    \n","    # w.r.t. w\n","    gw = x.T @ v\n","    \n","    # w.r.t. b\n","    gb = v.sum(axis=0)\n","    \n","    return gx, gw, gb\n","  \n","  return y, vjp\n","\n","@graph_computation\n","def sigmoid(x):\n","  y = 1/(1+(-x).exp())\n","  \n","  def vjp(v):\n","    dy = x.exp()/(1 + x.exp())**2\n","    return v*dy\n","  \n","  return y, vjp\n","\n","\n","@graph_computation\n","def relu(x):\n","  # y = torch.maximum(x, torch.zeros_like(x))\n","  y = torch.clamp(x, min=0)\n","\n","  def vjp(v):\n","    # Note that the torch implementation makes the gradient 1 at x=0\n","    return v * (x >= 0).float()\n","\n","  return y, vjp\n","\n","\n","@graph_computation\n","def square_error_loss(x, *, y):\n","  diff = x - y\n","  L = (diff**2).sum(axis=-1)\n","  \n","  def vjp(v):\n","    return v * 2 * diff\n","  \n","  return L, vjp\n","\n","@graph_computation\n","def tensor_sum(x):\n","  y = x.sum()\n","  \n","  def vjp(v):\n","    return v * torch.ones_like(x)\n","  \n","  return y, vjp\n","  \n","\n","@graph_computation\n","def tensor_concat(*tensors, axis=0):\n","  y = torch.cat(tensors, dim=axis)\n","\n","  def vjp(v):\n","    v_shape = v.shape\n","    grads = []\n","    j = axis if axis >= 0 else axis + len(v_shape)\n","    offset = 0\n","    fv = torch.reshape(v, (-1,)+v_shape[j:])  # if j == 0, then we add a dimension of size 1 to the front\n","    output_shape = v_shape[:j] + (-1,) + v_shape[j+1:]\n","    for t in tensors:\n","      width = t.shape[j]\n","      sl = v[:, offset:offset+width]\n","      sl = torch.reshape(sl, output_shape)\n","      grads.append(sl)\n","      offset += width\n","    return grads\n","\n","  return y, vjp\n","\n","\n","@graph_computation\n","def tensor_index(x, *, axis, index):\n","  if axis < 0:\n","    axis += len(x.shape)\n","  get_arg = (slice(None),)*axis + (index,)\n","  y = x[get_arg]\n","\n","  def vjp(v):\n","    r = torch.zeros_like(x)\n","    r[get_arg] = v\n","    return r\n","  \n","  return y, vjp"],"metadata":{"id":"RGwtNikW6wIi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test"],"metadata":{"id":"YqMbJLiX7Fhs"}},{"cell_type":"code","source":["# Compare to gradients calculated by torch\n","\n","def get_torch_grads(target, params):\n","  # zero out previous cum gradients\n","  for p in params:\n","    if p.grad is not None:\n","      p.grad.zero_()\n","  # update cum gradients\n","  target.backward(torch.ones_like(target), retain_graph=True)\n","  return [p.grad for p in params]"],"metadata":{"id":"PlF8sPezlQ4-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feed-forward network"],"metadata":{"id":"lbHkgazYknjl"}},{"cell_type":"code","source":["# build a simple NN with toy data\n","\n","# data - shape == (batch, features)\n","x = torch.tensor([[1,2,3], [4,5,6]], dtype=float, requires_grad=True)\n","y = torch.tensor([[1, 0], [0, 1]], dtype=float, requires_grad=True)\n","\n","# params\n","w1 = torch.tensor([[1, 1], [-1, 1], [-2, 2]], dtype=float, requires_grad=True)\n","b1 = torch.tensor([[0, 1]], dtype=float, requires_grad=True)\n","w2 = torch.tensor([[.2, .5], [.5, -.5]], dtype=float, requires_grad=True)\n","b2 = torch.tensor([[-1, .5]], dtype=float, requires_grad=True)\n","w3 = torch.tensor([[.7, -.5], [-.2, .3]], dtype=float, requires_grad=True)\n","b3 = torch.tensor([[.3, -.2]], dtype=float, requires_grad=True)\n","params = [w1, b1, w2, b2, w3, b3]\n","\n","# build NN\n","h0 = x\n","h1 = sigmoid(affine(h0, w1, b1))\n","h2 = relu(affine(h1, w2, b2))\n","h_out = affine(h2, w3, b3)\n","L = tensor_sum(square_error_loss(h_out, y=y))\n","L  # view the output"],"metadata":{"id":"CjXzqIHP6x9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# invoke our autodiff engine!\n","my_grads = L.backprop(params)\n","my_grads  # view our calculated gradients"],"metadata":{"id":"cU7BZdeb6zr1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compare to torch grads\n","torch_grads = get_torch_grads(L.output, params)\n","print('matches:', [torch.allclose(my_g, tc_g) for my_g, tc_g in zip(my_grads, torch_grads)])\n","# If all are True then we've succeeded"],"metadata":{"id":"Iy2bmrkK60-6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Recurrent network"],"metadata":{"id":"Fn8zNekakpuJ"}},{"cell_type":"code","source":["# build a recurrent NN with toy data\n","\n","# data - shape == (batch, time, features)\n","x = torch.tensor([[[1,2,3], [4,5,6], [7, 8, 9]], [[-1, 1, -2], [2, -3, 3], [-2, 3, -4]]], dtype=float, requires_grad=True)\n","y = torch.tensor([[1, 0], [0, 1]], dtype=float, requires_grad=True)\n","\n","# params\n","w1 = torch.tensor([[1, 1], [-1, 1], [-2, 2], [.5, -.5], [2, -2]], dtype=float, requires_grad=True)\n","b1 = torch.tensor([[0, 1]], dtype=float, requires_grad=True)\n","w2 = torch.tensor([[.2, .5], [.5, -.5]], dtype=float, requires_grad=True)\n","b2 = torch.tensor([[-1, .5]], dtype=float, requires_grad=True)\n","params = [w1, b1, w2, b2]\n","\n","# build NN\n","s = torch.zeros((2, 2))\n","for time_step in range(x.shape[1]):\n","  input = tensor_index(x, axis=1, index=time_step)\n","  input = tensor_concat(input, s, axis=-1)\n","  s = relu(affine(input, w1, b1))\n","final = affine(s, w2, b2)\n","L = tensor_sum(square_error_loss(final, y=y))\n","L  # view the output"],"metadata":{"id":"UBL21qrckrFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# invoke our autodiff engine!\n","my_grads = L.backprop(params + [x])\n","my_grads  # view our calculated gradients"],"metadata":{"id":"hlrtNpErl9P9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compare to torch grads\n","torch_grads = get_torch_grads(L.output, params + [x])\n","print('matches:', [torch.allclose(my_g, tc_g) for my_g, tc_g in zip(my_grads, torch_grads)])\n","# If all are True then we've succeeded"],"metadata":{"id":"FTPnU1A0mAum"},"execution_count":null,"outputs":[]}]}