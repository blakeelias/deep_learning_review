{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tensorflow_basics.ipynb","provenance":[{"file_id":"110wdlcNRbv9-WWiwCvT97BVQ4BEq2mk7","timestamp":1651180436647}],"collapsed_sections":[],"authorship_tag":"ABX9TyOl+7Cu9UbnGM77AuZk60CI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This is a demonstration of the \"raw\" TensorFlow 2.0 (eager) API.  \n","We get to see manual parameter creation, manual graph building, and manual gradient updates.  \n","After seeing how TF works, we can start throwing on helpers, wrappers, and managers for convenience and performance (like running operations on GPU).\n","\n","Reference: https://www.tensorflow.org/tutorials/quickstart/advanced"],"metadata":{"id":"ZiVtszEDnNNL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tourn-5CK0tN"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","print(\"TensorFlow version:\", tf.__version__)"]},{"cell_type":"markdown","source":["# Fetch the data"],"metadata":{"id":"zB5O7efRM9bK"}},{"cell_type":"code","source":["# Download MNIST dataset into numpy tensors.\n","\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","x_train = x_train.reshape(x_train.shape[0], -1) / 255\n","x_test = x_test.reshape(x_test.shape[0], -1) / 255\n","y_train = np.int_(y_train)\n","y_test = np.int_(y_test)"],"metadata":{"id":"Kp9Jm80ZLAGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# How big is our dataset? What kind of data do we have? \n","\n","print(x_train.shape, ', ', x_train.dtype)\n","print(y_train.shape, ', ', y_train.dtype)\n","print()\n","print(x_test.shape, ', ', x_test.dtype)\n","print(y_test.shape, ', ', y_test.dtype)"],"metadata":{"id":"nGpEg46gLRKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Look at an example\n","# Images are monochrome with integer pixel values between 0 and 255 (inclusive)\n","\n","np.set_printoptions(linewidth=1000)\n","print(x_train[0].reshape(28, 28))"],"metadata":{"id":"f-5f7I9dM7fx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize some images and check their labels\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","for i in range(6):\n","  print(y_train[i])\n","  plt.imshow(x_train[i].reshape(28, 28))\n","  plt.show()\n","  print('')"],"metadata":{"id":"yDMnZoy0NAuK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define the model"],"metadata":{"id":"kTkui1mdOpyF"}},{"cell_type":"code","source":["INPUT_DIM = x_train.shape[-1]\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = 10\n","\n","# Define and initialize the model parameters by sampling each element i.i.d. from a normal distribution\n","SCALE = 1e-2\n","W1 = tf.Variable(SCALE * np.random.normal(size=(INPUT_DIM, HIDDEN_DIM)))\n","W2 = tf.Variable(SCALE * np.random.normal(size=(HIDDEN_DIM, OUTPUT_DIM)))\n","B1 = tf.Variable(SCALE * np.random.normal(size=(1, HIDDEN_DIM)))\n","B2 = tf.Variable(SCALE * np.random.normal(size=(1, OUTPUT_DIM)))\n","\n","parameters = [W1, W2, B1, B2]\n","\n","@tf.function\n","def model_fn(x):\n","  # x.shape=(batch_size, INPUT_DIM)\n","  h = tf.nn.relu(tf.matmul(x, W1) + B1)\n","  return tf.matmul(h, W2) + B2"],"metadata":{"id":"sjkQbPmONYdN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test out our model\n","\n","out = model_fn(x_train[:23])\n","print(out.shape)"],"metadata":{"id":"ck5R2qXoWuVJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# View our parameters\n","\n","print(W1)\n","print(B1)"],"metadata":{"id":"w4KbBebKVS6C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the loss function\n","\n","@tf.function\n","def select_indices(x, indices):\n","  # From row i, select element indices[i], i.e.\n","  # return [x[i, indices[i]] for i in range(x.shape[0])]\n","\n","  # See https://stackoverflow.com/a/48491902/15601980\n","  row_indices = tf.range(indices.shape[0], dtype=tf.int64)\n","  full_indices = tf.stack([row_indices, indices], axis=1)\n","  return tf.gather_nd(x, full_indices)\n","\n","# Maximum likelihood loss (negative log probability of the data)\n","@tf.function\n","def mle_loss(logits, labels):\n","  # equivalent to cross entropy loss where target probs are 1 on the correct labels\n","  # https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy\n","  logits_adjusted = tf.nn.log_softmax(logits, axis=1)  # rescale outputs in log space\n","  return -tf.reduce_mean(select_indices(logits_adjusted, labels))"],"metadata":{"id":"OKWTl7eTXig3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["***Cross entropy loss***\n","\n","See https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy.\n","\n","Let $\\vec{q} = (q_1, \\dots, q_n)$ be a vector of predicted probabilities,  \n","and let $\\vec{p} = (p_1, \\dots, p_n)$ be a vector of target probabilities.\n","\n","The [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) of the two distributions is\n","\n","$$\n","H(\\vec{p}, \\vec{q}) = -\\sum_{i=1}^n p_i \\log q_i\n","$$\n","\n","Holding $\\vec{p}$ fixed, $H(\\vec{p}, \\vec{q})$ is maximized when $\\vec{q} = \\vec{p}$.\n","\n","In our case, the labels provide a one-hot target distribution.  \n","Let $t$ be the target label for some input image.\n","A one-hot distribution puts all probability on $t$, i.e.\n","\n","$$\\vec{\\mathbb{1}}[t] = (0,\\dots,0,1,0,\\dots,0)$$\n","\n","where $\\mathbb{1}[t]_t = 1$.\n","Let $\\vec{Y}$ be the raw model outputs and $\\vec{q}$ be the model probabilities, i.e. \n","\n","$$\n","\\vec{q} = \\text{softmax}(\\vec{Y}) = \\frac{1}{\\sum_{i=1}^n \\exp(Y_i)}\\Big(\\exp(Y_1),\\dots,\\exp(Y_n)\\Big)\n","$$\n","\n","and let $\\vec{p} = \\vec{\\mathbb{1}}[t]$.\n","\n","\n","Then the cross entropy loss (negated so that minimizing maximizes cross entropy) is\n","\n","\n","$$\\begin{aligned}\n","L &= -H(\\vec{p}, \\vec{q}) \\\\\n","&= -H\\left(\\vec{\\mathbb{1}}[t],\\ \\text{softmax}(\\vec{Y})\\right) \\\\\n","&= \\sum_{i=1}^n \\mathbb{1}[t]_i \\log\\left( \\text{softmax}(\\vec{Y}) \\right)\\\\\n","&= \\log\\left( \\text{softmax}(\\vec{Y})_t \\right) \\\\\n","&= Y_t - \\log\\left(\\sum_{i=1}^n \\exp(Y_i)\\right)\n","\\end{aligned}$$\n","\n"],"metadata":{"id":"kiLP8m-kZx3B"}},{"cell_type":"code","source":["@tf.function\n","def accuracy(logits, target):\n","  argmaxs = tf.math.argmax(logits, axis=1)\n","  corrects = tf.math.equal(argmaxs, target)\n","  return tf.math.count_nonzero(corrects) / logits.shape[0]"],"metadata":{"id":"J3RYqmJ0daFp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training loop"],"metadata":{"id":"jWXOaklsmSB2"}},{"cell_type":"code","source":["@tf.function\n","def train_step(images, labels, lr):\n","  with tf.GradientTape() as tape:\n","    predictions = model_fn(images)\n","    loss = mle_loss(predictions, labels)\n","    loss += regularizer_weight * sum(tf.norm(p, 1) for p in parameters)  # regularization\n","  gradients = tape.gradient(loss, parameters)\n","\n","  for p, g in zip(parameters, gradients):\n","    p.assign_sub(lr * g)\n","\n","  return loss"],"metadata":{"id":"Mo2qgfvlGNB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 100\n","learning_rate = 1e-2\n","num_epochs = 1000\n","regularizer_weight = 1e-2\n","\n","for epoch in range(batch_size):\n","  idx = np.random.permutation(x_train.shape[0])  # random ordering of the training set\n","\n","  ## training step\n","  for i in range(0, x_train.shape[0], batch_size):\n","    x = x_train[idx[i:i+batch_size]]\n","    y = y_train[idx[i:i+batch_size]]\n","\n","    loss_ = train_step(x, y, learning_rate).numpy()\n","\n","    if i % (batch_size * 100) == 0:\n","      print('Step:', i//batch_size, '; Loss:', loss_)\n","\n","  train_logits = model_fn(x_train)  # Training accurate\n","  test_logits = model_fn(x_test)  # Test accuracy\n","  print('')\n","  print('Epoch: %d | Train Accuracy: %.2f | Test Accuracy: %.2f' % (epoch, accuracy(train_logits, y_train), accuracy(test_logits, y_test)))\n","  print('')"],"metadata":{"id":"Aoma5Ny6dd9O"},"execution_count":null,"outputs":[]}]}