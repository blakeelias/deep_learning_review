{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_generation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMrHnvKV34rQ/uI8X/HwUKx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Goal: Build language model.\n","\n","References:\n","- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n","  - https://github.com/karpathy/char-rnn\n","  - https://cs.stanford.edu/people/karpathy/char-rnn/\n","  - https://gist.github.com/karpathy/587454dc0146a6ae21fc\n","- https://www.tensorflow.org/text/tutorials/text_generation"],"metadata":{"id":"nFaf1qGr8o4m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOH1tvW_8M9D"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","ks = tf.keras\n","print(\"TensorFlow version:\", tf.__version__)\n","\n","import urllib\n","import math"]},{"cell_type":"markdown","source":["# Get the data\n","\n","Using a Shakespeare dataset"],"metadata":{"id":"s0ndGP0X81Aw"}},{"cell_type":"code","source":["# Karpathy's datasets used in his blog post,\n","# http://karpathy.github.io/2015/05/21/rnn-effectiveness/,\n","# and listed here: https://cs.stanford.edu/people/karpathy/char-rnn/.\n","\n","TEXT_URL = {\n","    'shakespeare': 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt',\n","    'linux': 'https://cs.stanford.edu/people/karpathy/char-rnn/linux_input.txt',\n","    'tolstoy': 'https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt',\n","}['shakespeare']  # Select a dataset\n","\n","with urllib.request.urlopen(TEXT_URL) as f:\n","  text = f.read()\n","\n","print(f'Length of text: {len(text)} characters')"],"metadata":{"id":"U_neCQP__G7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note that the text is stored as a byte string\n","print(type(text))"],"metadata":{"id":"tk7-kuqkAuVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# look at sample of the data\n","print(text[2100:2600].decode(\"utf-8\"))"],"metadata":{"id":"Jql5loqL9Nh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This will be character level model. Serious language models use word pieces (https://paperswithcode.com/method/wordpiece).\n","\n","# Make a numpy array of ASCII chars\n","raw_seq = np.frombuffer(text, dtype=np.uint8)\n","\n","# Token ID to ascii code conversion\n","token_to_ascii = np.array(sorted(set(raw_seq)))\n","VOCAB_SIZE = len(token_to_ascii)\n","\n","# Ascii code to token ID conversion\n","ascii_to_token = np.full(256, -1, np.int_)\n","for token, ascii in enumerate(token_to_ascii):\n","  ascii_to_token[ascii] = token\n","\n","# Convert ascii array to token ID array\n","token_seq = ascii_to_token[raw_seq]\n","\n","print('vocab size:',VOCAB_SIZE)\n","print('seq:', token_seq[:20])\n","print(token_seq.shape)\n","print(token_seq.dtype)\n","print('\\ntoken_to_char:', token_to_ascii)\n","print('any invalid?', np.any(token_seq == -1))\n","print('min:', np.min(token_seq),'  max:', np.max(token_seq))"],"metadata":{"id":"MWft_7HC93gD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 32\n","CONTEXT_SIZE = 100  # truncated sequence length\n","PAD_CHAR = token_to_ascii[0]\n","PAD_LEN = math.ceil(token_seq.size / (BATCH_SIZE*CONTEXT_SIZE)) * BATCH_SIZE*CONTEXT_SIZE - seq.size\n","\n","parallel_seq = np.append(token_seq, [PAD_CHAR]*PAD_LEN).reshape(BATCH_SIZE, -1)\n","\n","# pad with beginning of sequences from next row\n","full_batches = 2  # How many full batches end of each row should bleed into start of next row\n","parallel_seq = np.concatenate((parallel_seq, np.roll(parallel_seq[:,:CONTEXT_SIZE*full_batches+1],-1,0)),1)\n","print('shape:', parallel_seq.shape)\n","\n","NUM_BATCHES = (parallel_seq.shape[1]-1) // CONTEXT_SIZE\n","print('num batches:', NUM_BATCHES)\n","print('assert',parallel_seq.size - NUM_BATCHES*BATCH_SIZE*CONTEXT_SIZE,'==',BATCH_SIZE)"],"metadata":{"id":"HDNyuppxRgOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_batch(batch_i, offset=0):\n","  # When offset==0 we have a training batch, and when offset==1 we have the training targets\n","  return parallel_seq[:, batch_i*CONTEXT_SIZE+offset: (batch_i+1)*CONTEXT_SIZE+offset]\n","\n","# get an example batch\n","print(get_batch(0))\n","print('')\n","print(get_batch(NUM_BATCHES-1))"],"metadata":{"id":"zP_jhrVDYRzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Human readable render of first training batch\n","[row.tobytes().decode('utf8') for row in token_to_ascii[get_batch(0)]]"],"metadata":{"id":"FPfC8iMMaL2m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show training targets for the above batch\n","[row.tobytes().decode('utf8') for row in token_to_ascii[get_batch(0, offset=1)]]"],"metadata":{"id":"dFqZx4W7aqVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Second to last training batch. Each line is now the next line down in the first batch\n","[row.tobytes().decode('utf8') for row in token_to_ascii[get_batch(NUM_BATCHES-2)]]"],"metadata":{"id":"IGq8bOkKZy54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define the model"],"metadata":{"id":"If--8WkYbitD"}},{"cell_type":"code","source":["# Define our model\n","\n","CELL_CLS = {\n","    'rnn': ks.layers.SimpleRNNCell,\n","    'lstm': ks.layers.LSTMCell,\n","    'gru': ks.layers.GRUCell,\n","}['lstm']\n","\n","class Model(ks.Model):\n","\n","  def __init__(self):\n","    super(Model, self).__init__()\n","    self.input_embed = ks.layers.Dense(100)\n","    self.cells = [CELL_CLS(200), CELL_CLS(150), CELL_CLS(100)]\n","    self.output_stack = [ks.layers.Dense(50, activation='relu'), ks.layers.Dense(VOCAB_SIZE)]\n","\n","  def call(self, x, s=None):\n","    # `x` is the input tensor and `s` is the hidden state\n","    # Expecting x.shape == (batch_size, context_size), where batch_size and context_size can be variable from run to run\n","    bs = tf.shape(x)[0]\n","    cs = tf.shape(x)[1]\n","    x = tf.one_hot(x, VOCAB_SIZE)  # shape == (batch_size, context_size, VOCAB_SIZE), where VOCAB_SIZE is a global constant\n","\n","    if s is None:\n","      s = [cell.get_initial_state(batch_size=bs, dtype=tf.float32) for cell in self.cells]\n","\n","    # Embed one-hot tokens\n","    e = self.input_embed(x)  # shape == (batch_size, context_size, 100)\n","\n","    # Recurrent cell stack\n","    outputs = []\n","    for h in tf.unstack(e, axis=1):  # loop over time within context window\n","      for l, cell in enumerate(self.cells):\n","        h, s[l] = cell(h, s[l])\n","      outputs.append(h)\n","\n","    # Feed forward stack\n","    h = tf.stack(outputs, axis=1)  # stack along the time axis\n","    for layer in self.output_stack:\n","      h = layer(h)\n","    return h, s"],"metadata":{"id":"wjJ9ZNyDbm1o","executionInfo":{"status":"ok","timestamp":1652108059465,"user_tz":300,"elapsed":194,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}}},"execution_count":104,"outputs":[]},{"cell_type":"code","source":["# Cross entropy loss.\n","# Same as https://github.com/danabo/deep_learning_review/blob/main/tensorflow_basics.ipynb\n","\n","@tf.function\n","def select_indices(x, indices):\n","  # From row i, select element indices[i], i.e.\n","  # return [x[i, indices[i]] for i in range(x.shape[0])]\n","  # Expect x.shape[:-1] == indices.shape\n","\n","  # See https://stackoverflow.com/a/48491902/15601980\n","  shape = tf.shape(x)\n","  x = tf.reshape(x, (-1, shape[-1]))  # Flatten all but last dimension\n","  indices = tf.reshape(indices, (-1,))\n","  row_indices = tf.range(tf.shape(indices)[0], dtype=tf.int64)\n","  full_indices = tf.stack([row_indices, indices], axis=1)\n","  return tf.reshape(tf.gather_nd(x, full_indices), shape[:-1])\n","\n","# Maximum likelihood loss (negative log probability of the data)\n","@tf.function\n","def mle_loss(logits, labels):\n","  # equivalent to cross entropy loss where target probs are 1 on the correct labels\n","  # https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy\n","  logits_adjusted = tf.nn.log_softmax(logits, axis=-1)  # rescale outputs in log space along last axis\n","  return -tf.reduce_mean(select_indices(logits_adjusted, labels))\n","\n","@tf.function\n","def accuracy(logits, target):\n","  argmaxs = tf.math.argmax(logits, axis=-1)\n","  corrects = tf.math.equal(argmaxs, target)\n","  return tf.reduce_mean(tf.cast(corrects, tf.float32))"],"metadata":{"id":"P3ARBoLEOrZk","executionInfo":{"status":"ok","timestamp":1652107638094,"user_tz":300,"elapsed":185,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}}},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":["# Training loop"],"metadata":{"id":"zzolK9SAS2uz"}},{"cell_type":"code","source":["learning_rate = 1e-3\n","\n","model = Model()\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,  # predictions will be given as logits (log unnormalized probabilities) rather than probabilities\n",")\n","\n","optimizer = tf.keras.optimizers.Adam()\n","\n","# Use GPU if available.\n","# https://www.tensorflow.org/guide/gpu\n","GPUs = tf.config.list_physical_devices('GPU')\n","device = '/GPU:0' if GPUs else '/CPU:0'\n","print('device =', device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yoouACXWS4qj","executionInfo":{"status":"ok","timestamp":1652108063383,"user_tz":300,"elapsed":162,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}},"outputId":"4bdac098-8233-485d-c902-bfe93a52fd01"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["device = /CPU:0\n"]}]},{"cell_type":"code","source":["@tf.function\n","def train_step(batch, labels, hidden_state=None):\n","  with tf.GradientTape() as tape:\n","    # training=True is only needed if there are layers with different\n","    # behavior during training versus inference (e.g. Dropout).\n","    logits, hidden_state = model(batch, hidden_state, training=True)\n","    loss = loss_object(labels, logits)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  return loss, logits, hidden_state\n","\n","\n","@tf.function\n","def accuracy(logits, target, normalize=True):\n","  argmaxs = tf.math.argmax(logits, axis=1)\n","  corrects = tf.math.equal(argmaxs, target)\n","  if normalize:\n","    return tf.reduce_mean(tf.cast(corrects, tf.float32))\n","  else:\n","    return tf.math.count_nonzero(corrects)"],"metadata":{"id":"mYfeiEJ7TAEd","executionInfo":{"status":"ok","timestamp":1652108064111,"user_tz":300,"elapsed":3,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}}},"execution_count":106,"outputs":[]},{"cell_type":"code","source":["num_epochs = 100\n","for epoch in range(num_epochs):\n","  # Performing truncated backprop through time (TBPTT).\n","  # Hidden states are carried over between batches, but gradients are not propagated beyond a batch.\n","  # At the end of each epoch the hidden state is reset to its default (typically all zeros).\n","  hidden_state = None  # None tells the model to use the default hidden state\n","  for batch_i in range(NUM_BATCHES):  \n","    # Move tensors to the configured device\n","    batch = get_batch(batch_i)\n","    labels = get_batch(batch_i, offset=1)\n","    with tf.device(device):\n","      loss_, logits_, hidden_state = train_step(batch, labels, hidden_state)\n","\n","    if i % 100 == 0:\n","      print('  Step: %d | Train Loss: %.4f | Train Accuracy: %.2f' % (i, loss_.numpy(), accuracy(logits_, labels).numpy()))\n","\n","  # Save model checkpoint\n","  model.save(f'./training_checkpoints/ckpt_{epoch}')\n","\n","  print('')\n","  print('Finished epoch')\n","  print('')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":244},"id":"LM3tdrGNTMwc","executionInfo":{"status":"error","timestamp":1652108197143,"user_tz":300,"elapsed":175,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}},"outputId":"0577f651-1155-42ad-b00d-605aee572c44"},"execution_count":108,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-108-7143d72679a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Move tensors to the configured device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'NUM' is not defined"]}]},{"cell_type":"code","source":["# TODO: generate text"],"metadata":{"id":"GokikcjoWUIw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load checkpoint\n","\n","Reference: https://www.tensorflow.org/guide/keras/save_and_serialize"],"metadata":{"id":"vCfo1d7kZ_eJ"}},{"cell_type":"code","source":["%ls training_checkpoints"],"metadata":{"id":"uB5WAypRZtdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_copy = ks.models.load_model('./training_checkpoints/ckpt_0')\n","model_copy.compile()"],"metadata":{"id":"EFj7yQ9OaD12"},"execution_count":null,"outputs":[]}]}