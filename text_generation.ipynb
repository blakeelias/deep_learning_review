{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPksxlOkKWhsNdg0a+kaWjf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Goal: Build language model.\n","\n","References:\n","- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n","  - https://github.com/karpathy/char-rnn\n","  - https://cs.stanford.edu/people/karpathy/char-rnn/\n","  - https://gist.github.com/karpathy/587454dc0146a6ae21fc\n","- https://www.tensorflow.org/text/tutorials/text_generation"],"metadata":{"id":"nFaf1qGr8o4m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOH1tvW_8M9D"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","ks = tf.keras\n","print(\"TensorFlow version:\", tf.__version__)\n","\n","import urllib\n","import math\n","from collections import namedtuple"]},{"cell_type":"code","source":["# I define `dot_product_attention` in this notebook below. No need to import it.\n","\n","# Install the tensor2tensor library which contains useful functions for the attention mechanism.\n","## !pip3 install tensor2tensor\n","## from tensor2tensor.layers.common_attention import dot_product_attention"],"metadata":{"id":"RPr_ITM-VBKn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get and process the data\n","\n","Using a Shakespeare dataset"],"metadata":{"id":"s0ndGP0X81Aw"}},{"cell_type":"code","source":["# Karpathy's datasets used in his blog post,\n","# http://karpathy.github.io/2015/05/21/rnn-effectiveness/,\n","# and listed here: https://cs.stanford.edu/people/karpathy/char-rnn/.\n","\n","TEXT_URL = {\n","    'shakespeare': 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt',\n","    'linux': 'https://cs.stanford.edu/people/karpathy/char-rnn/linux_input.txt',\n","    'tolstoy': 'https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt',\n","}['shakespeare']  # Select a dataset\n","\n","with urllib.request.urlopen(TEXT_URL) as f:\n","  text = f.read()\n","\n","print(f'Length of text: {len(text)} characters')"],"metadata":{"id":"U_neCQP__G7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note that the text is stored as a byte string\n","print(type(text))"],"metadata":{"id":"tk7-kuqkAuVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# look at sample of the data\n","print(text[2100:2600].decode(\"utf-8\"))"],"metadata":{"id":"Jql5loqL9Nh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This will be character level model. Serious language models use word pieces (https://paperswithcode.com/method/wordpiece).\n","\n","# Make a numpy array of ASCII chars\n","raw_seq = np.frombuffer(text, dtype=np.uint8)\n","\n","# Token ID to ascii code conversion\n","token_to_ascii = np.array(sorted(set(raw_seq)))\n","VOCAB_SIZE = len(token_to_ascii)\n","\n","# Ascii code to token ID conversion\n","ascii_to_token = np.full(256, -1, np.int_)\n","for token, ascii in enumerate(token_to_ascii):\n","  ascii_to_token[ascii] = token\n","\n","# Convert ascii array to token ID array\n","token_seq = ascii_to_token[raw_seq]\n","\n","print('vocab size:',VOCAB_SIZE)\n","print('seq:', token_seq[:20])\n","print(token_seq.shape)\n","print(token_seq.dtype)\n","print('\\ntoken_to_char:', token_to_ascii)\n","print('any invalid?', np.any(token_seq == -1))\n","print('min:', np.min(token_seq),'  max:', np.max(token_seq))"],"metadata":{"id":"MWft_7HC93gD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 32  # 16\n","CONTEXT_SIZE = 100  #  500;  truncated sequence length\n","PAD_CHAR = token_to_ascii[0]\n","PAD_LEN = math.ceil(token_seq.size / (BATCH_SIZE*CONTEXT_SIZE)) * BATCH_SIZE*CONTEXT_SIZE - token_seq.size\n","\n","parallel_seq = np.append(token_seq, [PAD_CHAR]*PAD_LEN).reshape(BATCH_SIZE, -1)\n","\n","# pad with beginning of sequences from next row\n","full_batches = 2  # How many full batches end of each row should bleed into start of next row\n","parallel_seq = np.concatenate((parallel_seq, np.roll(parallel_seq[:,:CONTEXT_SIZE*full_batches+1],-1,0)),1)\n","print('shape:', parallel_seq.shape)\n","\n","NUM_BATCHES = (parallel_seq.shape[1]-1) // CONTEXT_SIZE\n","print('num batches:', NUM_BATCHES)\n","print('assert',parallel_seq.size - NUM_BATCHES*BATCH_SIZE*CONTEXT_SIZE,'==',BATCH_SIZE)"],"metadata":{"id":"HDNyuppxRgOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_batch(batch_i, offset=0):\n","  # When offset==0 we have a training batch, and when offset==1 we have the training targets\n","  return parallel_seq[:, batch_i*CONTEXT_SIZE+offset: (batch_i+1)*CONTEXT_SIZE+offset]\n","\n","# get an example batch\n","print(get_batch(0))\n","print('')\n","print(get_batch(NUM_BATCHES-1))"],"metadata":{"id":"zP_jhrVDYRzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Human readable render of first training batch\n","[row.tobytes().decode('utf8') for row in token_to_ascii[get_batch(0)]]"],"metadata":{"id":"FPfC8iMMaL2m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show training targets for the above batch\n","[row.tobytes().decode('utf8') for row in token_to_ascii[get_batch(0, offset=1)]]"],"metadata":{"id":"dFqZx4W7aqVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Second to last training batch. Each line is now the next line down in the first batch\n","[row.tobytes().decode('utf8') for row in token_to_ascii[get_batch(NUM_BATCHES-2)]]"],"metadata":{"id":"IGq8bOkKZy54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define the model"],"metadata":{"id":"If--8WkYbitD"}},{"cell_type":"code","source":["# Define our model\n","\n","# TODO: implement cells myself\n","CELL_CLS = {\n","    'rnn': ks.layers.SimpleRNNCell,\n","    'lstm': ks.layers.LSTMCell,\n","    'gru': ks.layers.GRUCell,\n","}['lstm']\n","\n","class RNN(ks.Model):\n","\n","  def __init__(self, use_cnn=False):\n","    super(RNN, self).__init__()\n","    self.cells = [CELL_CLS(100)]  # , CELL_CLS(50)]\n","    self.embedding_size = 20\n","    self.input_embed = ks.layers.Dense(self.embedding_size)\n","    self.output_stack = [ks.layers.Dense(VOCAB_SIZE)]\n","    self.conv1d = ks.layers.Conv1D(filters=self.embedding_size, kernel_size=4, padding='causal')  # 'causal' convolutions only depend on inputs to the left (and center) of the current position\n","    self.use_cnn = use_cnn\n","\n","  def call(self, x, s=None, cache=None):\n","    # `x` is the input tensor.\n","    # `s` is thee initial recurrent state. If None then a zero tensor is used.\n","\n","    # Expecting x.shape == (batch_size, context_size), where batch_size and context_size can be variable from run to run\n","    bs, cs = tf.unstack(tf.shape(x))\n","    x = tf.one_hot(x, VOCAB_SIZE)  # shape == (batch_size, context_size, VOCAB_SIZE), where VOCAB_SIZE is a global constant\n","\n","    if s is None:\n","      s = [cell.get_initial_state(batch_size=bs, dtype=tf.float32) for cell in self.cells]\n","    else:\n","      s = list(s)  # Make a copy of the input list since we will modify it in place\n","\n","    # Embed one-hot tokens\n","    e = self.input_embed(x)  # shape == (batch_size, context_size, embedding_size)\n","    # Note: ks.layers.Embedding does the same thing but more efficiently for large vocabularies\n","\n","    if self.use_cnn:\n","      # 1D convolution across time puts neighbor information into each embedding in the sequence \n","      e = self.conv1d(e)\n","\n","    # Recurrent cell stack\n","    outputs = []\n","    # loop over time\n","    for t, h in enumerate(tf.unstack(e, axis=1)):\n","      # loop over cells\n","      for l, cell in enumerate(self.cells):\n","        h, s[l] = cell(h, s[l])\n","      outputs.append(h)\n","\n","    # Feed forward stack\n","    h = tf.stack(outputs, axis=1)  # stack along the time axis\n","    for layer in self.output_stack:\n","      h = layer(h)\n","    return h, s, None  # logits, final hidden state, cache"],"metadata":{"id":"wjJ9ZNyDbm1o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training loop"],"metadata":{"id":"zzolK9SAS2uz"}},{"cell_type":"code","source":["learning_rate = 1e-3\n","\n","model = RNN(use_cnn=True)\n","# model = AttentionRNN(use_attn=True, use_cnn=True)\n","# model = Transformer(d_model=128, num_layers=4, num_attn_heads=8)\n","\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,  # predictions will be given as logits (log unnormalized probabilities) rather than probabilities\n",")\n","\n","# optimizer = tf.keras.optimizers.SGD(learning_rate, momentum=0.0)\n","optimizer = tf.keras.optimizers.Adam(learning_rate)\n","\n","# Use GPU if available.\n","# https://www.tensorflow.org/guide/gpu\n","GPUs = tf.config.list_physical_devices('GPU')\n","device = '/GPU:0' if GPUs else '/CPU:0'\n","print('device =', device)"],"metadata":{"id":"yoouACXWS4qj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(batch, labels, state=None):\n","  with tf.GradientTape() as tape:\n","    # training=True is only needed if there are layers with different\n","    # behavior during training versus inference (e.g. Dropout).\n","    logits, state_out, _ = model(batch, state, training=True)\n","    loss = loss_object(labels, logits)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  return loss, logits, state_out\n","\n","\n","@tf.function\n","def accuracy(logits, target, normalize=True):\n","  argmaxs = tf.math.argmax(logits, axis=-1)\n","  corrects = tf.math.equal(argmaxs, target)\n","  if normalize:\n","    return tf.reduce_mean(tf.cast(corrects, tf.float32))\n","  else:\n","    return tf.math.count_nonzero(corrects)"],"metadata":{"id":"mYfeiEJ7TAEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%rm -rf logs"],"metadata":{"id":"f4Un6dJFPDeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start tensorboard (optional)\n","# This will embed a tensorboard front-end in the output of this cell, which will display training graphs in realtime.\n","# See https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_in_notebooks.ipynb\n","%load_ext tensorboard\n","%tensorboard --logdir logs"],"metadata":{"id":"cf0aQmG5Ave7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tb_writer = tf.summary.create_file_writer('logs')  # Tensorboard writer\n","global_step = 0"],"metadata":{"id":"vvgTbfT9Aw9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 100\n","for epoch in range(num_epochs):\n","  # Performing truncated backprop through time (TBPTT).\n","  # States are carried over between batches, but gradients are not propagated beyond a batch.\n","  # At the end of each epoch the state is reset to its default (typically all zeros).\n","  state = None  # None tells the model to use the default state\n","  for batch_i in range(NUM_BATCHES):  \n","    # Move tensors to the configured device\n","    batch = get_batch(batch_i)\n","    labels = get_batch(batch_i, offset=1)\n","    with tf.device(device):\n","      loss_, logits_, state = train_step(batch, labels, state)\n","    \n","    global_step += 1\n","\n","    if batch_i % 10 == 0:\n","      loss_ = loss_.numpy()\n","      acc_ = accuracy(logits_, labels).numpy()\n","      print('  Step: %d out of %d | Train Loss: %.4f | Train Accuracy: %.2f' % (batch_i, NUM_BATCHES, loss_, acc_))\n","      with tb_writer.as_default():\n","        tf.summary.scalar('train_loss', loss_, step=global_step)\n","        tf.summary.scalar('train_accuracy', acc_, step=global_step)\n","\n","  # Save model checkpoint\n","  # model.save(f'./training_checkpoints/ckpt_{epoch}')\n","\n","  print('')\n","  print('Finished epoch')\n","  print('')"],"metadata":{"id":"LM3tdrGNTMwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Manual save model\n","model.save(f'./training_checkpoints/ckpt_{epoch}')"],"metadata":{"id":"LSMHJoR0aleE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inspect predictions\n","batch_num = 1000\n","input_num = 0\n","l, _, _ = model(get_batch(batch_num, offset=0))\n","p = tf.nn.softmax(l[input_num], axis=-1).numpy()\n","print('predictions:')\n","print(np.argmax(p, axis=1))\n","print(token_to_ascii[np.argmax(p, axis=1)].tobytes().decode('utf8'))\n","print('\\ntargets:')\n","print(get_batch(batch_num, offset=1)[input_num])\n","print(token_to_ascii[get_batch(batch_num, offset=1)[input_num]].tobytes().decode('utf8'))"],"metadata":{"id":"aVWiY654d4fD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load checkpoint\n","\n","Reference: https://www.tensorflow.org/guide/keras/save_and_serialize"],"metadata":{"id":"vCfo1d7kZ_eJ"}},{"cell_type":"code","source":["%ls training_checkpoints"],"metadata":{"id":"uB5WAypRZtdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_copy = ks.models.load_model('./training_checkpoints/ckpt_0')\n","model_copy.compile()"],"metadata":{"id":"EFj7yQ9OaD12"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate text"],"metadata":{"id":"Ym5L7e-YXJF4"}},{"cell_type":"code","source":["MODE = 'sample'  # 'argmax'\n","GENERATE_LENGTH = 100\n","PROMPT = \"\"\"ROMEO:\"\"\"\n","\n","# Sampling temperature.\n","# Lower temperature means peakier distribution.\n","# As the temp goes to 0, the distribution approaches one-hot (equivalent to taking the argmax)\n","temp = .1\n","\n","prompt = np.frombuffer(bytes(PROMPT, 'utf-8'), dtype=np.uint8)\n","prompt = ascii_to_token[prompt]\n","x = prompt[None, :]  # Last token in prompt is the input to the first generating step\n","_, state, cache = model(x[:, :-1], training=False)  # get state and cache for prompt\n","for n in range(GENERATE_LENGTH):\n","  logits, state, cache = model(x[:, -1:], state, cache, training=False)\n","  if MODE == 'sample':\n","    next = tf.random.categorical(logits[:, -1]/temp, num_samples=1)\n","  else:  # MODE == 'argmax'\n","    next = tf.math.argmax(logits[:, -1:], axis=-1)\n","  x = np.concatenate((x, next.numpy()), axis=1)"],"metadata":{"id":"4h2QvWlWYElG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(token_to_ascii[x].tobytes().decode('utf8'))"],"metadata":{"id":"JFH4uDp_bgv4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: implement beam search"],"metadata":{"id":"jxqjaALlXob6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fancy Language Models\n","\n","Additional bells, whistles, and \"exotic\" auto-regressive architectures:\n","- Attention + recurrent cells\n","- Transformer (attention only)\n","- WaveNet (convolution only)"],"metadata":{"id":"Hi9lLqok9HDk"}},{"cell_type":"markdown","source":["Scaled dot-product attention from [Attention Is All You Need](https://arxiv.org/abs/1706.03762):\n","\n","$$\n","\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n","$$\n","\n","where $Q$ is a matrix with shape $s_q \\times d_q$,  \n","$K$ with shape $s_k \\times d_q$,  \n","and $V$ with shape $s_k \\times d_v$,  \n","with $s_q, s_k$ the sequence lengths of queries and keys,  \n","and $d_q, d_v$ are the vector lengths (depths) of queries (and keys) and values.\n","\n","    \n","$QK^T$ has shape $s_q \\times s_k$  \n","and $\\text{Attention}(Q, K, V)$ has shape $s_q \\times d_v$."],"metadata":{"id":"xGOCY6citS-V"}},{"cell_type":"code","source":["# Dot product attention is used in \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762), which introduces the transformer.\n","\n","# This implementation is based on tensor2tensor.layers.common_attention.dot_product_attention,\n","# https://github.com/tensorflow/tensor2tensor/blob/ef1fccebe8d2c0cf482f41f9d940e2938c816c78/tensor2tensor/layers/common_attention.py#L1602\n","# and the Transformer tutorial,\n","# https://www.tensorflow.org/text/tutorials/transformer#scaled_dot_product_attention\n","\n","def dot_product_attention(q, k, v, mask=None, scaled=True, return_weights=False):\n","  # q.shape == (..., seq_len_q, depth_qk)\n","  # k.shape == (..., seq_len_kv, depth_qk)\n","  # v.shape == (..., seq_len_kv, depth_v)\n","  # mask.shape == (..., seq_len_q, seq_len_kv)\n","  # mask is a binary tensor, with 0 entries being removed from attention.\n","\n","  # For each query vector in the query sequence q, we dot product with every key\n","  # vector in the key sequence k to get attention weights, one for each value\n","  # vector in the value sequence v. We return the weighted average of the value\n","  # vectors. Each query vector in the query sequence produces an average value\n","  # vector, averaged over the same value sequence.\n","\n","  # If q, k, v have higher ranks than 2, `dot_product_attention` is performed\n","  # element-wise along the preceding tensor dimensions.\n","\n","  # Attention logits are unscaled log-weights \n","  # logits == [dot(vec_q, vec_k) for (vec_q, vec_k) in zip(q, k)]\n","  logits = tf.matmul(q, k, transpose_b=True)  # shape == (..., seq_len_q, seq_len_kv)\n","\n","  if scaled:\n","    # scale logits by 1/sqrt(depth_qk)\n","    depth_qk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    logits = logits / tf.math.sqrt(depth_qk)\n","\n","  # Mask out sequence elements by zeroing out their attention weights.\n","  # Attention weights are zero if their corresponding logits are -inf.\n","  if mask is not None:\n","    # Use a large negative finite number to avoid getting NaNs.\n","    logits = tf.where(mask, logits, tf.experimental.numpy.full_like(logits, -1e9))\n","\n","  # Turn attention logits into non-negative weights that sum to 1\n","  weights = tf.nn.softmax(logits, axis=-1)  # shape == (..., seq_len_q, seq_len_kv)\n","\n","  # Take weighted average\n","  average_v = tf.matmul(weights, v)  # shape == (..., seq_len_q, depth_v)\n","\n","  if return_weights:\n","    return average_v, weights\n","  return average_v"],"metadata":{"id":"tDzaob7CpZML"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CELL_CLS = {\n","    'rnn': ks.layers.SimpleRNNCell,\n","    'lstm': ks.layers.LSTMCell,\n","    'gru': ks.layers.GRUCell,\n","}['lstm']"],"metadata":{"id":"lDMVnL6OEjPL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Attention + recurrent cells"],"metadata":{"id":"iHMENZ5tPIEg"}},{"cell_type":"code","source":["# RNN cell on top of an attention layer.\n","\n","class AttentionRNN(ks.Model):\n","\n","  def __init__(self, use_attn=False, use_cnn=False):\n","    super(AttentionRNN, self).__init__()\n","    self.embedding_size = 20\n","    self.input_embed = ks.layers.Dense(self.embedding_size)\n","    self.cells = [CELL_CLS(100)]  # , CELL_CLS(50)]\n","    self.output_stack = [ks.layers.Dense(VOCAB_SIZE)]\n","    self.conv1d = ks.layers.Conv1D(filters=self.embedding_size, kernel_size=4, padding='causal')  # 'causal' convolutions only depend on inputs to the left (and center) of the current position\n","    self.num_attn_heads = 10\n","    self.query_embed = ks.layers.Dense(self.num_attn_heads * self.embedding_size//2)  # self.embedding_size//2 is the size of the query and key vectors\n","    self.use_attn = use_attn\n","    self.use_cnn = use_cnn\n","\n","  def call(self, x, s=None, cache=None):\n","    # `x` is the input tensor and `s` is the recurrent state.\n","\n","    # Expecting x.shape == (batch_size, context_size), where batch_size and context_size can be variable from run to run\n","    bs, cs = tf.unstack(tf.shape(x))\n","    x = tf.one_hot(x, VOCAB_SIZE)  # shape == (batch_size, context_size, VOCAB_SIZE), where VOCAB_SIZE is a global constant\n","\n","    if s is None:\n","      s = [cell.get_initial_state(batch_size=bs, dtype=tf.float32) for cell in self.cells]\n","    else:\n","      s = list(s)  # Make a copy of the input list since we will modify it in place\n","\n","    # Embed one-hot tokens\n","    e = self.input_embed(x)  # shape == (batch_size, context_size, embedding_size)\n","    # Note: ks.layers.Embedding does the same thing but more efficiently for large vocabularies\n","\n","    if self.use_cnn:\n","      # 1D convolution across time puts neighbor information into each embedding in the sequence \n","      e = self.conv1d(e)\n","\n","    if self.use_attn:\n","      # Split embedding dimension into two sectors: key and value.\n","      # That gives us a key and value pair for each timestep.\n","      num_features = tf.shape(e)[-1]\n","      k = e[:, :, :num_features//2]\n","      v = e[:, :, num_features//2:]\n","      k_size = tf.shape(k)[-1]  # should equal self.embedding_size//2\n","\n","      # kv caching\n","      if cache is not None:\n","        # prepend cached k and v to new k and v\n","        k_cache, v_cache = cache\n","        k = tf.concat((k_cache, k), axis=1)\n","        v = tf.concat((v_cache, v), axis=1)\n","        cache_len = tf.shape(k_cache)[1]  # time dimension\n","      else:\n","        cache_len = 0\n","      \n","      # cache full k and v\n","      cache = (k, v)\n","\n","    # Recurrent cell stack\n","    outputs = []\n","    # loop over time\n","    for t, h in enumerate(tf.unstack(e, axis=1)):\n","      if self.use_attn:\n","        # Query is computed from the current input and recurrent states.\n","        query_context = tf.concat(tf.nest.flatten([h, s]), axis=1)\n","        q = tf.reshape(self.query_embed(query_context), (bs, self.num_attn_heads, k_size))\n","\n","        # We slice `k` and `v` so that the future is not included\n","        attn_result = dot_product_attention(q, k[:, :cache_len+t+1], v[:, :cache_len+t+1])\n","        #    , bias=None, make_image_summary=False)  # Additional arguments for tensor2tensor dot_product_attention\n","\n","        # attn_result shape is (batch_size, self.num_attn_heads, k_size)\n","        attn_result = tf.reshape(attn_result, (bs, -1))  # flatten last two dims\n","        h = tf.concat((h, attn_result), axis=-1)  # Concat current step input embedding with attention result\n","\n","      # loop over cells\n","      for l, cell in enumerate(self.cells):\n","        h, s[l] = cell(h, s[l])\n","      outputs.append(h)\n","\n","    # Feed forward stack\n","    h = tf.stack(outputs, axis=1)  # stack along the time axis\n","    for layer in self.output_stack:\n","      h = layer(h)\n","    return h, s, cache"],"metadata":{"id":"I75GVnzY9GKP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer (attention only)"],"metadata":{"id":"aOG9ffntPMjx"}},{"cell_type":"code","source":["# Multi-head attention layer.\n","# This implementation is based on https://www.tensorflow.org/text/tutorials/transformer#multi-head_attention\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","\n","  def __init__(self, *, d_out, d_attn, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_attn = d_attn\n","    self.d_out = d_out\n","\n","    self.d_concat = d_attn * num_heads\n","\n","    self.w_q = tf.keras.layers.Dense(self.d_concat)\n","    self.w_k = tf.keras.layers.Dense(self.d_concat)\n","    self.w_v = tf.keras.layers.Dense(self.d_concat)\n","    self.w_o = tf.keras.layers.Dense(self.d_out)\n","\n","  def split_heads(self, x):\n","    # x.shape == (batch_size, seq_len, d_concat)\n","    # Split the last dimension into (num_heads, d_attn).\n","    # Transpose the result such that the shape is (batch_size, num_heads, seq_len, d_attn)\n","    batch_size, seq_len, _ = tf.unstack(tf.shape(x))\n","    x = tf.reshape(x, (batch_size, seq_len, self.num_heads, self.d_attn))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","  def call(self, q, k, v, mask):\n","    # q.shape == (batch_size, seq_len, d_q)\n","    # k.shape == (batch_size, seq_len, d_k)\n","    # v.shape == (batch_size, seq_len, d_v)\n","    batch_size = tf.shape(q)[0]\n","\n","    q = self.w_q(q)  # (batch_size, seq_len, d_concat)\n","    k = self.w_k(k)  # (batch_size, seq_len, d_concat)\n","    v = self.w_v(v)  # (batch_size, seq_len, d_concat)\n","\n","    q = self.split_heads(q)  # (batch_size, num_heads, seq_len_q, d_attn)\n","    k = self.split_heads(k)  # (batch_size, num_heads, seq_len_k, d_attn)\n","    v = self.split_heads(v)  # (batch_size, num_heads, seq_len_v, d_attn)\n","    # For our purposes seq_len_q == seq_len_k == seq_len_v, but it's insightful\n","    # to see how these variables propagate if they were not equal.\n","\n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, d_attn)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    attn_avg, attn_weights = dot_product_attention(q, k, v, mask, return_weights=True)\n","    attn_avg = tf.transpose(attn_avg, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, d_attn)\n","    attn_avg_concat = tf.reshape(attn_avg, (batch_size, -1, self.d_concat))  # (batch_size, seq_len_q, d_concat)\n","\n","    output = self.w_o(attn_avg_concat)  # (batch_size, seq_len_q, d_out)\n","\n","    return output, attn_weights"],"metadata":{"id":"-6fvj9JDm1As"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copied from https://www.tensorflow.org/text/tutorials/transformer#create_the_transformer_model\n","\n","def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates\n","\n","\n","def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","\n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","  pos_encoding = angle_rads[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)"],"metadata":{"id":"P4ekTNrsS4v4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The transformer, i.e. self-attention, is introduced in \"Attention Is All You Need\" (https://arxiv.org/abs/1706.03762).\n","\n","# We are only interested in the self-attntion decoder, like the one used in \n","# GPT-1 (https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)\n","# and \"Generating wikipedia by summarizing long sequences\" (https://arxiv.org/abs/1801.10198).\n","\n","# This implementation is based on\n","# https://www.tensorflow.org/text/tutorials/transformer#decoder_layer\n","# https://www.tensorflow.org/text/tutorials/transformer#decoder\n","# and https://www.tensorflow.org/text/tutorials/transformer#create_the_transformer_model\n","\n","\n","SelfAttnLayer = namedtuple('SelfAttnLayer', 'mha,layernorm1,dropout1,ff,layernorm2,dropout2')\n","\n","\n","class Transformer(ks.Model):\n","  # Maximum supported sequence length.\n","  # Make this large enough to handle evaluation inputs.\n","  MAX_SEQ_LENGTH = 10*CONTEXT_SIZE\n","\n","\n","  def __init__(self, d_model, num_layers, num_attn_heads, dropout_rate=0.1):\n","    super(Transformer, self).__init__()\n","\n","    self.pos_encoding = positional_encoding(self.MAX_SEQ_LENGTH, d_model)\n","    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","\n","    self.dense_bottom = tf.keras.layers.Dense(d_model)\n","\n","    self.self_attention_layers = [\n","        SelfAttnLayer(\n","            mha = MultiHeadAttention(d_out=d_model, d_attn=d_model // num_attn_heads, num_heads=num_attn_heads),\n","            layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6),\n","            dropout1 = tf.keras.layers.Dropout(dropout_rate),\n","\n","            ff = tf.keras.Sequential([\n","                tf.keras.layers.Dense(256, activation='relu'),  # (batch_size, seq_len, d_ff)\n","                tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","            ]),\n","            layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6),\n","            dropout2 = tf.keras.layers.Dropout(dropout_rate),\n","        )\n","        for _ in range(num_layers)\n","    ]\n","\n","    self.dense_top = tf.keras.layers.Dense(VOCAB_SIZE)\n","\n","  def call(self, x, state=None, cache=None, training=False, return_attn_weights=False):\n","    # x.shape == (batch_size, seq_len)\n","\n","    # Expecting x.shape == (batch_size, seq_len), where batch_size and seq_len can be variable from run to run\n","    bs, seq_len = tf.unstack(tf.shape(x))\n","    x = tf.one_hot(x, VOCAB_SIZE)  # shape == (batch_size, seq_len, VOCAB_SIZE), where VOCAB_SIZE is a global constant\n","    x = self.dense_bottom(x)  # shape == (batch_size, seq_len, d_model)\n","\n","    # get the number of time steps stored in the cache\n","    if cache is not None:\n","      cache_seq_len = tf.shape(cache[0])[1]\n","    else:\n","      cache_seq_len = 0\n","\n","    look_ahead_mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n","    look_ahead_mask = tf.concat((tf.ones((seq_len, cache_seq_len)), look_ahead_mask), axis=1)\n","    look_ahead_mask = tf.cast(look_ahead_mask, tf.bool)\n","    look_ahead_mask = tf.expand_dims(tf.expand_dims(look_ahead_mask, axis=0), axis=0)  # shape == (1, 1, seq_len, seq_len + cache_seq_len)\n","    \n","    # self-attention decoder\n","    attention_weights = {}\n","    x += self.pos_encoding[:, cache_seq_len:cache_seq_len+seq_len, :]\n","    x = self.dropout(x, training=training)\n","\n","    output_cache = []\n","    for i, (mha, layernorm1, dropout1, ff, layernorm2, dropout2) in enumerate(self.self_attention_layers):\n","      # kv caching\n","      q = x  # keep queries separate\n","      if cache is not None:\n","        kv = tf.concat((cache[i], x), axis=1)  # concat along time axis\n","      else:\n","        kv = x\n","      output_cache.append(kv)\n","\n","      attn_result, attn_weights_block = mha(q, kv, kv, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","      attn_result = dropout1(attn_result, training=training)\n","      # residual connction and layer norm\n","      x = layernorm1(attn_result + x)  # (batch_size, target_seq_len, d_model)\n","\n","      ff_output = ff(x)  # (batch_size, target_seq_len, d_model)\n","      ff_output = dropout2(ff_output, training=training)\n","      x = layernorm2(ff_output + x)  # (batch_size, target_seq_len, d_model)\n","\n","      attention_weights[f'decoder_layer{i+1}_block'] = attn_weights_block\n","    \n","    # dense layer on output\n","    final_output = self.dense_top(x)  # (batch_size, tar_seq_len, target_vocab_size)\n","\n","    return_val = (final_output, None, output_cache)\n","    if return_attn_weights:\n","      return_val += (attention_weights,)\n","    return return_val"],"metadata":{"id":"Nqe83fr3F3dq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## WaveNet (convolution only)"],"metadata":{"id":"wXduZTBSPTT8"}},{"cell_type":"code","source":["# TODO"],"metadata":{"id":"zQCon87-PWC0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1t5M0VstrjlZ"},"execution_count":null,"outputs":[]}]}