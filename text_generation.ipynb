{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_generation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOROv6V7JR8ZbVkZUEZcA3x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Goal: Build language model.\n","\n","References:\n","- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n","  - https://github.com/karpathy/char-rnn\n","  - https://cs.stanford.edu/people/karpathy/char-rnn/\n","  - https://gist.github.com/karpathy/587454dc0146a6ae21fc\n","- https://www.tensorflow.org/text/tutorials/text_generation"],"metadata":{"id":"nFaf1qGr8o4m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dOH1tvW_8M9D"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","ks = tf.keras\n","print(\"TensorFlow version:\", tf.__version__)\n","\n","import urllib\n","import math"]},{"cell_type":"code","source":["# Install the tensor2tensor library which contains useful functions for the attention mechanism.\n","!pip3 install tensor2tensor\n","from tensor2tensor.layers.common_attention import dot_product_attention"],"metadata":{"id":"RPr_ITM-VBKn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get the data\n","\n","Using a Shakespeare dataset"],"metadata":{"id":"s0ndGP0X81Aw"}},{"cell_type":"code","source":["# Karpathy's datasets used in his blog post,\n","# http://karpathy.github.io/2015/05/21/rnn-effectiveness/,\n","# and listed here: https://cs.stanford.edu/people/karpathy/char-rnn/.\n","\n","TEXT_URL = {\n","    'shakespeare': 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt',\n","    'linux': 'https://cs.stanford.edu/people/karpathy/char-rnn/linux_input.txt',\n","    'tolstoy': 'https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt',\n","}['shakespeare']  # Select a dataset\n","\n","with urllib.request.urlopen(TEXT_URL) as f:\n","  text = f.read()\n","\n","print(f'Length of text: {len(text)} characters')"],"metadata":{"id":"U_neCQP__G7D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Note that the text is stored as a byte string\n","print(type(text))"],"metadata":{"id":"tk7-kuqkAuVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# look at sample of the data\n","print(text[2100:2600].decode(\"utf-8\"))"],"metadata":{"id":"Jql5loqL9Nh_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This will be character level model. Serious language models use word pieces (https://paperswithcode.com/method/wordpiece).\n","\n","# Make a numpy array of ASCII chars\n","raw_seq = np.frombuffer(text, dtype=np.uint8)\n","\n","# Token ID to ascii code conversion\n","token_to_ascii = np.array(sorted(set(raw_seq)))\n","VOCAB_SIZE = len(token_to_ascii)\n","\n","# Ascii code to token ID conversion\n","ascii_to_token = np.full(256, -1, np.int_)\n","for token, ascii in enumerate(token_to_ascii):\n","  ascii_to_token[ascii] = token\n","\n","# Convert ascii array to token ID array\n","token_seq = ascii_to_token[raw_seq]\n","\n","print('vocab size:',VOCAB_SIZE)\n","print('seq:', token_seq[:20])\n","print(token_seq.shape)\n","print(token_seq.dtype)\n","print('\\ntoken_to_char:', token_to_ascii)\n","print('any invalid?', np.any(token_seq == -1))\n","print('min:', np.min(token_seq),'  max:', np.max(token_seq))"],"metadata":{"id":"MWft_7HC93gD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 32\n","CONTEXT_SIZE = 100  # truncated sequence length\n","PAD_CHAR = token_to_ascii[0]\n","PAD_LEN = math.ceil(token_seq.size / (BATCH_SIZE*CONTEXT_SIZE)) * BATCH_SIZE*CONTEXT_SIZE - token_seq.size\n","\n","parallel_seq = np.append(token_seq, [PAD_CHAR]*PAD_LEN).reshape(BATCH_SIZE, -1)\n","\n","# pad with beginning of sequences from next row\n","full_batches = 2  # How many full batches end of each row should bleed into start of next row\n","parallel_seq = np.concatenate((parallel_seq, np.roll(parallel_seq[:,:CONTEXT_SIZE*full_batches+1],-1,0)),1)\n","print('shape:', parallel_seq.shape)\n","\n","NUM_BATCHES = (parallel_seq.shape[1]-1) // CONTEXT_SIZE\n","print('num batches:', NUM_BATCHES)\n","print('assert',parallel_seq.size - NUM_BATCHES*BATCH_SIZE*CONTEXT_SIZE,'==',BATCH_SIZE)"],"metadata":{"id":"HDNyuppxRgOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_batch(batch_i, offset=0):\n","  # When offset==0 we have a training batch, and when offset==1 we have the training targets\n","  return parallel_seq[:, batch_i*CONTEXT_SIZE+offset: (batch_i+1)*CONTEXT_SIZE+offset]\n","\n","# get an example batch\n","print(get_batch(0))\n","print('')\n","print(get_batch(NUM_BATCHES-1))"],"metadata":{"id":"zP_jhrVDYRzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Human readable render of first training batch\n","[row.tobytes().decode('utf8') for row in token_to_ascii[get_batch(0)]]"],"metadata":{"id":"FPfC8iMMaL2m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show training targets for the above batch\n","[row.tobytes().decode('utf8') for row in token_to_ascii[get_batch(0, offset=1)]]"],"metadata":{"id":"dFqZx4W7aqVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Second to last training batch. Each line is now the next line down in the first batch\n","[row.tobytes().decode('utf8') for row in token_to_ascii[get_batch(NUM_BATCHES-2)]]"],"metadata":{"id":"IGq8bOkKZy54"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Define the model"],"metadata":{"id":"If--8WkYbitD"}},{"cell_type":"code","source":["# Define our model\n","\n","CELL_CLS = {\n","    'rnn': ks.layers.SimpleRNNCell,\n","    'lstm': ks.layers.LSTMCell,\n","    'gru': ks.layers.GRUCell,\n","}['lstm']\n","\n","class Model(ks.Model):\n","\n","  def __init__(self, use_attn=False, use_cnn=False):\n","    super(Model, self).__init__()\n","    self.embedding_size = 20\n","    self.input_embed = ks.layers.Dense(self.embedding_size)\n","    self.cells = [CELL_CLS(100)]  # , CELL_CLS(50)]\n","    self.output_stack = [ks.layers.Dense(VOCAB_SIZE)]\n","    self.conv1d = ks.layers.Conv1D(filters=self.embedding_size, kernel_size=4, padding='same')\n","    self.num_attn_heads = 10\n","    self.query_embed = ks.layers.Dense(self.num_attn_heads * self.embedding_size//2)  # self.embedding_size//2 is the size of the query and key vectors\n","    self.use_attn = use_attn\n","    self.use_cnn = use_cnn\n","\n","  def call(self, x, s=None, more_context=None):\n","    # `x` is the input tensor and `s` is the recurrent state\n","    # `more_context` is an optional tensor with shape (batch_size, extra_context_size).\n","    #     It is used to give the attention mechanism additional timesteps to look at.\n","    #     The context window for the attention is then the time-axis concatenation of\n","    #     `more_context`  and `x`, i.e. `tf.concat(more_context, x, axis=1)`.\n","\n","    # Expecting x.shape == (batch_size, context_size), where batch_size and context_size can be variable from run to run\n","    bs, cs = tf.unstack(tf.shape(x))\n","    x = tf.one_hot(x, VOCAB_SIZE)  # shape == (batch_size, context_size, VOCAB_SIZE), where VOCAB_SIZE is a global constant\n","\n","    if s is None:\n","      s = [cell.get_initial_state(batch_size=bs, dtype=tf.float32) for cell in self.cells]\n","    else:\n","      s = list(s)  # Make a copy of the input list since we will modify it in place\n","\n","    # Embed one-hot tokens\n","    e = self.input_embed(x)  # shape == (batch_size, context_size, embedding_size)\n","    # Note: ks.layers.Embedding does the same thing but more efficiently for large vocabularies\n","\n","    if self.use_cnn:\n","      # 1D convolution across time puts neighbor information into each embedding in the sequence \n","      e = self.conv1d(e)  # TODO: check that feature and time dims are in correct order\n","\n","    if self.use_attn:\n","      if more_context is None:\n","        extra_cs = 0\n","        full_context = e\n","      else:\n","        more_context = tf.one_hot(more_context, VOCAB_SIZE)\n","        extra_cs = tf.shape(more_context)[1]  # size of time dim on more_context\n","        full_context = tf.concat((self.input_embed(more_context), e), axis=1)\n","      # Split embedding dimension into two sectors: key and value.\n","      # That gives us a key and value pair for each timestep.\n","      num_features = tf.shape(full_context)[-1]\n","      k = full_context[:, :, :num_features//2]\n","      v = full_context[:, :, num_features//2:]\n","      k_size = tf.shape(k)[-1]  # should equal self.embedding_size//2\n","\n","    # Recurrent cell stack\n","    outputs = []\n","    for t, h in enumerate(tf.unstack(e, axis=1)):\n","      if self.use_attn:\n","        # Query is computed from the current input and recurrent states.\n","        query_context = tf.concat(tf.nest.flatten([h, s]), axis=1)\n","        q = tf.reshape(self.query_embed(query_context), (bs, self.num_attn_heads, k_size))\n","        # https://github.com/tensorflow/tensor2tensor/blob/c8fe559e0b357389d8754474e1306b6ca9afc4f3/tensor2tensor/layers/common_attention.py#L1602\n","        # We slice `k` and `v` so that the future is not included\n","        attn_result = dot_product_attention(q, k[:, :extra_cs+t+1], v[:, :extra_cs+t+1], bias=None, make_image_summary=False)\n","        # attn_result shape is (batch_size, self.num_attn_heads, k_size)\n","        attn_result = tf.reshape(attn_result, (bs, -1))  # flatten last two dims\n","        h = tf.concat((h, attn_result), axis=-1)  # Concat current step input embedding with attention result\n","      for l, cell in enumerate(self.cells):\n","        h, s[l] = cell(h, s[l])\n","      outputs.append(h)\n","\n","    # Feed forward stack\n","    h = tf.stack(outputs, axis=1)  # stack along the time axis\n","    for layer in self.output_stack:\n","      h = layer(h)\n","    return h, s"],"metadata":{"id":"wjJ9ZNyDbm1o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training loop"],"metadata":{"id":"zzolK9SAS2uz"}},{"cell_type":"code","source":["learning_rate = 1e-3\n","\n","# model = Model()\n","model = Model(use_attn=True, use_cnn=True)\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,  # predictions will be given as logits (log unnormalized probabilities) rather than probabilities\n",")\n","\n","optimizer = tf.keras.optimizers.Adam()\n","\n","# Use GPU if available.\n","# https://www.tensorflow.org/guide/gpu\n","GPUs = tf.config.list_physical_devices('GPU')\n","device = '/GPU:0' if GPUs else '/CPU:0'\n","print('device =', device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yoouACXWS4qj","executionInfo":{"status":"ok","timestamp":1652144043777,"user_tz":300,"elapsed":166,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}},"outputId":"be558682-412e-4e5f-b58d-bfaacc89ffd0"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["device = /GPU:0\n"]}]},{"cell_type":"code","source":["@tf.function\n","def train_step(batch, labels, state=None):\n","  with tf.GradientTape() as tape:\n","    # training=True is only needed if there are layers with different\n","    # behavior during training versus inference (e.g. Dropout).\n","    logits, state_out = model(batch, state, training=True)\n","    loss = loss_object(labels, logits)\n","  gradients = tape.gradient(loss, model.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","  return loss, logits, state_out\n","\n","\n","@tf.function\n","def accuracy(logits, target, normalize=True):\n","  argmaxs = tf.math.argmax(logits, axis=-1)\n","  corrects = tf.math.equal(argmaxs, target)\n","  if normalize:\n","    return tf.reduce_mean(tf.cast(corrects, tf.float32))\n","  else:\n","    return tf.math.count_nonzero(corrects)"],"metadata":{"id":"mYfeiEJ7TAEd","executionInfo":{"status":"ok","timestamp":1652144045565,"user_tz":300,"elapsed":180,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}}},"execution_count":87,"outputs":[]},{"cell_type":"code","source":["num_epochs = 100\n","for epoch in range(num_epochs):\n","  # Performing truncated backprop through time (TBPTT).\n","  # States are carried over between batches, but gradients are not propagated beyond a batch.\n","  # At the end of each epoch the state is reset to its default (typically all zeros).\n","  state = None  # None tells the model to use the default state\n","  for batch_i in range(NUM_BATCHES):  \n","    # Move tensors to the configured device\n","    batch = get_batch(batch_i)\n","    labels = get_batch(batch_i, offset=1)\n","    with tf.device(device):\n","      loss_, logits_, state = train_step(batch, labels, state)\n","\n","    if batch_i % 100 == 0:\n","      print('  Step: %d out of %d | Train Loss: %.4f | Train Accuracy: %.2f' % (batch_i, NUM_BATCHES, loss_.numpy(), accuracy(logits_, labels).numpy()))\n","\n","  # Save model checkpoint\n","  model.save(f'./training_checkpoints/ckpt_{epoch}')\n","\n","  print('')\n","  print('Finished epoch')\n","  print('')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":654},"id":"LM3tdrGNTMwc","executionInfo":{"status":"error","timestamp":1652144142852,"user_tz":300,"elapsed":96432,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}},"outputId":"38e05b7b-6f7f-406a-b258-9dfd3f0aaf84"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["  Step: 0 | Train Loss: 4.1994 | Train Accuracy: 0.02\n","  Step: 100 | Train Loss: 2.6124 | Train Accuracy: 0.34\n","  Step: 200 | Train Loss: 0.7230 | Train Accuracy: 0.85\n","  Step: 300 | Train Loss: 0.2393 | Train Accuracy: 0.96\n","  Step: 400 | Train Loss: 0.1309 | Train Accuracy: 0.98\n","  Step: 500 | Train Loss: 0.0764 | Train Accuracy: 0.99\n","  Step: 600 | Train Loss: 0.0664 | Train Accuracy: 0.99\n","  Step: 700 | Train Loss: 0.0594 | Train Accuracy: 0.99\n","  Step: 800 | Train Loss: 0.0450 | Train Accuracy: 0.99\n","  Step: 900 | Train Loss: 0.0470 | Train Accuracy: 0.99\n","  Step: 1000 | Train Loss: 0.0350 | Train Accuracy: 0.99\n","  Step: 1100 | Train Loss: 0.0349 | Train Accuracy: 0.99\n","  Step: 1200 | Train Loss: 0.0334 | Train Accuracy: 0.99\n","  Step: 1300 | Train Loss: 0.0320 | Train Accuracy: 0.99\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-88-3a30044deccf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mloss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Save model\n","model.save(f'./training_checkpoints/ckpt_{epoch}')"],"metadata":{"id":"LSMHJoR0aleE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Inspect predictions\n","\n","l, s = model(get_batch(1000, offset=0))"],"metadata":{"id":"aVWiY654d4fD","executionInfo":{"status":"ok","timestamp":1652144253861,"user_tz":300,"elapsed":908,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","source":["p = tf.nn.softmax(l[0], axis=-1).numpy()"],"metadata":{"id":"5mTY9gBneLkL","executionInfo":{"status":"ok","timestamp":1652144309434,"user_tz":300,"elapsed":142,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["np.argmax(p, axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WN231vpeY0q","executionInfo":{"status":"ok","timestamp":1652144915206,"user_tz":300,"elapsed":182,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}},"outputId":"221cfde2-5942-4ef6-8df6-29aaf6bcf5e3"},"execution_count":106,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([48, 41, 59, 60,  1, 59, 56, 55, 51, 45, 54,  1, 63, 55, 58, 44, 59,\n","       12,  0,  0, 31, 21, 15, 21, 26, 21, 33, 31, 10,  0, 27,  1, 42, 52,\n","       45, 59, 59, 45, 44,  1, 48, 45, 41, 62, 45, 54, 59,  2,  0,  0, 34,\n","       27, 24, 33, 25, 26, 21, 13, 10,  0, 25, 55, 58, 45,  1, 54, 55, 42,\n","       52, 45,  1, 42, 52, 55, 63, 59,  1, 60, 48, 41, 54,  1, 45, 62, 45,\n","       58,  1, 60, 48, 55, 61,  1, 63, 49, 59, 45,  1, 63, 55, 61])"]},"metadata":{},"execution_count":106}]},{"cell_type":"code","source":["get_batch(1000, offset=1)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5owJRyHefrJ","executionInfo":{"status":"ok","timestamp":1652144343876,"user_tz":300,"elapsed":163,"user":{"displayName":"Daniel Abolafia","userId":"04951539501050032931"}},"outputId":"93277f18-57b6-498b-f87e-24d12d7c4f79"},"execution_count":102,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([48, 41, 59, 60,  1, 59, 56, 55, 51, 45, 54,  1, 63, 55, 58, 44, 59,\n","       12,  0,  0, 31, 21, 15, 21, 26, 21, 33, 31, 10,  0, 27,  1, 42, 52,\n","       45, 59, 59, 45, 44,  1, 48, 45, 41, 62, 45, 54, 59,  2,  0,  0, 34,\n","       27, 24, 33, 25, 26, 21, 13, 10,  0, 25, 55, 58, 45,  1, 54, 55, 42,\n","       52, 45,  1, 42, 52, 55, 63, 59,  1, 60, 48, 41, 54,  1, 45, 62, 45,\n","       58,  1, 60, 48, 55, 61,  1, 63, 49, 59, 45,  1, 63, 55, 58])"]},"metadata":{},"execution_count":102}]},{"cell_type":"markdown","source":["## Load checkpoint\n","\n","Reference: https://www.tensorflow.org/guide/keras/save_and_serialize"],"metadata":{"id":"vCfo1d7kZ_eJ"}},{"cell_type":"code","source":["%ls training_checkpoints"],"metadata":{"id":"uB5WAypRZtdl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_copy = ks.models.load_model('./training_checkpoints/ckpt_0')\n","model_copy.compile()"],"metadata":{"id":"EFj7yQ9OaD12"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate text"],"metadata":{"id":"Ym5L7e-YXJF4"}},{"cell_type":"code","source":["MODE = 'argmax'  # 'argmax'\n","GENERATE_LENGTH = 1000\n","PROMPT = \"\"\"\n","ROMEO:\"\"\"\n","\n","prompt = np.frombuffer(bytes(PROMPT, 'utf-8'), dtype=np.uint8)\n","prompt = ascii_to_token[prompt]\n","_, state = model(prompt[None,:-1])  # Process prompt and get resulting recurrent state\n","generated = prompt[None,:]  # Last token in prompt is the input to the first generating step\n","state = None\n","for n in range(GENERATE_LENGTH):\n","  logits, state = model(generated[:, -1:], state, more_context=generated[:, :-1])\n","  if MODE == 'sample':\n","    next = tf.random.categorical(logits[:, 0], num_samples=1)\n","  else:  # MODE == 'argmax'\n","    next = tf.math.argmax(logits, axis=-1)\n","  generated = np.concatenate((generated, next.numpy()), axis=1)"],"metadata":{"id":"4h2QvWlWYElG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(token_to_ascii[generated].tobytes().decode('utf8'))"],"metadata":{"id":"JFH4uDp_bgv4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: implement beam search"],"metadata":{"id":"jxqjaALlXob6"},"execution_count":null,"outputs":[]}]}